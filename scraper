#!/usr/bin/env ruby

# vim scripts scraping monster
# This code is Copyright (c) 2010 Scott Bronson
# Released under the MIT License.
#
# DEPENDENCIES
# This script requires Ruby 1.9.2.
# Make sure you have unzip, unrar, 7za, and xz installed.
#   Ubuntu: sudo apt-get install unzip unrar p7zip-full xz-utils
#   Macintosh: sudo port install unrar p7zip xz
# Also, for gems:
#   Ubuntu: sudo apt-get install libxml2-dev libxslt1-dev zlib1g-dev libbz2-dev libcurl4-openssl-dev
#   Fedora: sudo yum install bzip2-devel libxml2-devel libxslt-devel libcurl-devel
#
# FULL SCRAPE:
# To start a full scrape, do this:
#   rm -f state.json
#   FOREVER=1 ./scraper
# Now the scraper is warm and ready to perform continuous RSS scraping.
#
# AUTOMATIC RSS Scrape:
# Run the scraper with no args to perform an rss scrape.  It downloads
# new scripts in the rss feed and remembers its position for next time.
#     ./scraper
#
# MANUAL Debugging:
# - with a positive number, does a full scrape / compile / upload cycle
#     ./scraper 987
#     ./scraper $(seq 1001 2000)
# - with a negative number, scrapes but does not compile or upload
#     ./scraper -987
# - with a .json file, compiles the git repo
#     ./scraper scripts/0987*
# - with a bare git repo, pushes the repo up to github
#     ./scraper repos/0987*
#
# TESTING:
# You may think that this script has no tests.  Not true!  The run-test
# script creates a repo for every script in the system, runs git log on
# it, and dumps the results.  If anything has changed, you'll see it.
#   JOBS=4 ./run-test stock
#   cd result/stats-stock
#   git status; git diff; etc.
# JOBS (optional): splits args into N groups and runs them in parallel.
#
# Because authors are always renaming scripts, changing email addrs and
# readmes, and deleting revisions, a from-scratch scrape will always
# be different from the repos in github.  Therefore, you should do this
# after every few scrapes:
#   ./scraper --dump
# In the scraper-stats repo, the master branch contains the ongoing repos
# and the test branch contains the recreate-from-scratch each time repos.
#
# RECOMPILING:
# Sometimes the scraper generates bad repos but you don't discover this
# until after they've been pushed to github.  No problem!  Create a
# text file containing the names of the repos that need fixing and  then
# delete and recreate them:
#   ./delete-repos $(cat BADNAMES)
#   FOREVER=1 ./scraper $(cat BADNAMES)
# If there's a network or github error, just rerun the command until it
# succeeds.
#
# RESCUING:
# If you deleted a script repo and want to restore your local copy
# with the one on github:
#    git clone --bare git://github.com/vim-scripts/SCRIPTNAME
#    ./scraper SCRIPTID      # to restore internal state
# Now the script is exactly the same as if it had been scraped
# locally from the start.


$:.unshift './lib'

require 'rubygems'
require 'bundler'
Bundler.require

require 'hpricot'             # hpricot gem
require 'open-uri'
require 'cgi'
require 'json'                # json gem
require 'zlib'
require 'bzip2'               # bzip2-ruby gem
require 'mime/types'
require 'mimemagic'           # mimemagic gem
require 'tmpdir'
require 'tempfile'
require 'find'
require 'octokit'             # octokit gem
require 'hashie'              # hashie gem
require 'htmlentities'        # htmlentities gem
require 'feedzirra'           # feedzirra gem
require 'erubis'              # erubis gem
require 'mail'                # mail gem
require 'fileutils'
require 'open3'
require 'retryable'

require 'github'
require 'gitrepo'


include Retryable
# :on => [] means that we won't retry anything unless the caller
# specifies the exact exceptions that it wants to be retried.
retryable_options :detect_nesting => true, :tries => 4,
  :sleep => lambda { |n| 4 ** n }, :on => []

# This is the name and email address of the git committer.
$vimscripts_name = "Able Scraper"
$vimscripts_email = 'scraper@vim-scripts.org'

$rss_url = 'https://feed43.com/vim-scripts.xml'
$idle_count = 10    # number of scripts to scrape when we have nothing else to do
$max_run_time = 14.minutes + 30.seconds    # script exits normally once this time has elapsed
$repos_dir = ENV['REPOS_DIR'] || 'repos'        # if a repo has been renamed there will be duplicate ids in here
$scripts_dir = 'scripts'    # should never be any duplicate ids in here
$packages_dir = 'packages'
$webcache_dir = 'webcache'
$git_script_file = 'vim-script.json'   # the file in the bare repo that stores the script that generated it
$state_file = 'state.json'             # keeps track of what mode we're in (full vs. rss scrape)
$pushing = true             # set this to false to prevent pushing (normally, if we find changes in a repo, we push)
$ignore_cache = false       # we pull from the webcache if we can, setting to true forces pulling from the network

$vimdirs = %w{after autoload bin compiler colors doc ftdetect ftplugin indent keymap plugin syntax}
$textext = %w{au3 bat c cpp csh diff h klip patch pl pm ps py rb set sh snip snippet snippets tcl txt xml vim vim.orig}.map { |x| "\\.#{x}$" }.join('|')

# vim.org doesn't offer any way to delete a script so people have invented all sorts of ways of doing it.
# 1022 2668 2730, and 3080 have also been deleted but I give up.  This is good enough.
# 3519 is simply an obsolete copy of jquery and really should be deleted
$deleted_scripts = %w{364 548 549 550 762 1032 1129 1263 1280 1295 1301 1430 1436 1452 1509 1562 1669 1789 1824 1949 2056 2172 2306 2309 2313 2316 2318 2323 2352 2456 2498 2664 2861 3076 3080 3519}

# not sure why an author would leave a corrupt package on vim scripts forever but owell.
# also we can't trust the script version and date pair to be unique: script 2709 SudoEdit.vim
$skip_packages = {
  1609 =>  ["2006-10-06 3.8", "2006-10-13 3.9", "2006-11-07 4.1.0", "2006-11-08 4.2.0", "2006-11-18 4.3.0"],
  3075 => ["2010-07-28 0.12", "2010-07-28 0.11"],
}

# some scripts were created with the wrong type.  this fixes the ones that matter.
#     https://groups.google.com/group/vim_use/msg/6f9f82e8c6fb4faa
# note that you must re-scrape after adding a fix ("./scraper 1780").
# regenerating ("./scraper scripts/1780*") is not sufficient in this case.
$script_type_fixes = {
  93   => 'ftplugin',
  1780 => 'syntax',
}

# if the regex matches the path, it is run through gsub and the result
# used as the new path.  If the replace string is nil, the file is
# suppressed.   Doesn't work for gifs and a few others (easy to fix).
$file_location_fixes = {
  284  => {      /([^\/]+\.vim)$/ => 'ftplugin/tex/\1' }, # all .vim files go in ftplugin/tex
  1095 => { /^.*\/([^\/]+\.vim)$/ => 'ftplugin/tex/\1' }, # all .vim files go in ftplugin/tex
  1771 => { /readme$/ => nil                           }, # has identical README and readme files.
  2651 => { /^(.*)Syntax(.*)$/ => '\1syntax\2'         }, # breaks on case sensitive filesystems
  3027 => { /^root\/\.vim\/(.*)$/ => '\1'              }, # grsecurity balled up his entire root dir
}

# Version nazi
raise "Must run under Ruby 1.9.2" unless RUBY_VERSION == "1.9.2"


Dir.mkdir $scripts_dir unless test ?d, $scripts_dir
Dir.mkdir $repos_dir unless test ?d, $repos_dir
Dir.mkdir $packages_dir unless test ?d, $packages_dir
Dir.mkdir $webcache_dir unless test ?d, $webcache_dir

# this mime magic is far too vague.  it false-triggers all the time.
MimeMagic.remove 'application/x-gmc-link'


class ScrapeError < RuntimeError; end       # retryable problem when scraping
class SourceForgeError < ScrapeError; end   # sourceforge being stupid
class NoContentError < ScrapeError; end     # page appears to be rendered incorrectly

# Turns out Ruby isn't very good about limiting the types of errors
# we need to handle...  These are the ones that make sense to retry.
def retryable_errors
  [
    ScrapeError,
    Errno::ECONNRESET,
    Timeout::Error,
    Errno::ETIMEDOUT,   # Connection timed out - connect(2) (Errno::ETIMEDOUT)
    OpenURI::HTTPError,
    SocketError,        # getaddrinfo: Name or service not known (SocketError)
    GitRepo::GitError,
  ]
end


# https://github.com/hpricot/hpricot/issues#issue/25
# super ugly that hpricot manages to screw up charset encodings so badly.

# hopefully the next version of hpricot allows us to get rid of these monkeypatches
module Hpricot
  module Traverse
    def inner_text
      if respond_to?(:children) and children
        children.map { |x| str = x.inner_text;
          str = str.dup.force_encoding('ISO-8859-1').encode('UTF-8') if str.encoding.to_s == 'ASCII-8BIT' || str.encoding.to_s == 'ISO-8859-1';
          str
        }.join
      else
        ""
      end
    end
  end

  def self.uxs(str)
    str = str.to_s
    str = str.dup.force_encoding('ISO-8859-1').encode('UTF-8') if str.encoding.to_s == 'ASCII-8BIT' || str.encoding.to_s == 'ISO-8859-1'
    str.gsub(/\&(\w+);/) { [NamedCharacters[$1] || 63].pack("U*") }.
      gsub(/\&\#(\d+);/) { [$1.to_i].pack("U*") }
  end
end

# and and hopefully a future version will allow us to remove these monkeypatches
module Hpricot
  module Traverse
    def inner_content
      if respond_to?(:children) and children
        children.map { |x| x.inner_content }.join
      else
        ''
      end
    end
  end

  class Elements < Array
    def inner_content
      map { |x| x.inner_content }.join
    end
  end

  class Text
    def inner_content
      str = content.to_s
      str = str.force_encoding('ISO-8859-1').encode("UTF-8") if str.encoding.to_s == 'ASCII-8BIT'
      CGI.unescapeHTML(HTMLEntities.new.decode(str.gsub(/&nbsp;/, " ")))
    end
  end

  class CData
    alias_method :inner_content, :content
  end
end


# There's a bizarre bug when passing a Hashie::Mash to JSON.pretty_generate.
# See https://github.com/bronson/whose-bug for an attempt to track it down.
# This routine just converts to plain hashes before generating.
def json_pretty arg
  arg = arg.to_hash if arg.kind_of? Hashie::Mash
  JSON.pretty_generate(arg) + "\n"
end


# When running normally, we can be in one of two states: rss and full.
# rss is when the rss is synced and we can update only the scripts that have changed.
# full is when we're just starting or we have lost the rss sync.

def read_state
  json = JSON.parse(File.read($state_file)) rescue nil
  Hashie::Mash.new json
end


def write_state state
  File.open($state_file, 'w') { |f| f.write json_pretty(state) }
end


# vim.org has added a new email obfuscation trick: replacing @ and . with images.
def unfuddle_email elem
  elem.search('img[@src*=emailat]' ).each { |e| e.swap("@") }
  elem.search('img[@src*=emaildot]').each { |e| e.swap(".") }
  elem.inner_text
end


def cached_open url, retries
  path = File.join $webcache_dir, filenameify(url)
  # ignore_cache should be true if we're running a real scrape and should
  # never use stale data.  Otherwise, leave it false so your testing and
  # debugging will just use the local file system.
  uri = URI.parse('http://something.example.com/directory/')
  OpenSSL::SSL::SSLSocket#hostname = uri.host
  if $ignore_cache || !File.exist?(path) || File.size(path) <= 0
    File.open(path, 'w') do |f|
      puts "downloading #{url}#{retries > 0 ? "  RETRY #{retries}" : ""}"
      open(url) { |u| f.write(u.read) }
    end
  end
  File.open(path) { |f| yield f }
end


def scrape_page url, retries
  doc = cached_open(url, retries) { |f| Hpricot(f) }

  # vim.org (SourceForge) seems to have a bug where every thousand requests or so
  # it returns a page indicating that it's lost its head.  No big deal, immediately
  # retrying always seems to fix it.
  if doc.search('title').inner_text == "Unknown Site"
    puts "Sourceforge lost its head for #{url} on try #{retries}, trying again."
    raise SourceForgeError.new "Sourceforge blew #{url}"
  end

  return doc
end


def scrape_author user_id, retries
  $authors ||= []
  unless $authors[user_id.to_i]
    doc = scrape_page "https://www.vim.org/account/profile.php?user_id=#{user_id}", retries
    unless doc.at('td[text()="user name"]')
      puts "    no content received #{retries}"
      raise NoContentError.new "bad page"
    end

    u = {'user_id' => user_id }
    u['user_name'] = doc.at('td[text()="user name"]').next_sibling.inner_content
    u['first_name'] = doc.at('td[text()="first name"]').next_sibling.inner_content
    u['last_name'] = doc.at('td[text()="last name"]').next_sibling.inner_content
    u['email'] = unfuddle_email doc.at('td[text()="email"]').next_sibling
    u['homepage'] = doc.at('td[text()="homepage"]').next_sibling.inner_content
    $authors[user_id.to_i] = u
  end
  return $authors[user_id.to_i]
end


def script_id_to_url(script_id)
  "https://www.vim.org/scripts/script.php?script_id=#{script_id}"
end


def script_id_from_url(url)
  url =~ /[?&;]script_id=(\d+)/ or raise "Could not parse a script id from <<#{url}>>"
  $1
end


def scrape_script(script_id)
  script_id = script_id.to_s

  if $deleted_scripts.include? script_id
    puts "Skipped #{script_id} -- deleted."
    return nil
  end

  s = {'script_id' => script_id}
  doc = nil
  retryable(:on => retryable_errors) do |retries|
    doc = scrape_page script_id_to_url(script_id), retries

    if doc.search('title').inner_text == "Error : vim online"
      puts "Skipped #{script_id} -- doesn't exist."
      return nil
    end

    s['display_name'], s['summary'] = doc.search('.txth1').inner_content.split(" : ", 2)
    unless s['display_name']
      puts "    no content received #{retries}"
      raise NoContentError.new "bad page"
    end
  end

  s['name'] = githubify(s['display_name'])
  s['script_type'] = $script_type_fixes[script_id.to_i] ||
  doc.at('td[text()="script type"]').parent.next_sibling.children.first.inner_content

  desc = doc.at('td[text()="description"]').parent.next_sibling.children.first
  desc.search('br').each do |br|
    # restore the newline to every element preceeding a br.
    prev = br.previous;
    if prev && prev.text?
      prev.content = prev.content + "\r" unless prev.content.end_with?("\r")
    else
      br.before "\r"
    end
  end
  s['description'] = desc.inner_content.gsub("\r", "\n")

  if false     # we don't use this info and it generates too much noise
    doc.search('td.lightbg~td').find { |e| e.inner_text =~ /Rating.*\s(-?\d+)\/(\d+),.*Downloaded[^\d]*(\d+)/m }
    s['rating_total'], s['rating_votes'], s['downloads'] = $1, $2, $3      # https://www.vim.org/karma.php
  end

  s['install_details'] = doc.at('td[text()="install details"]').parent.next_sibling.children.first.inner_content.gsub("\r", "\n")
  # reject links with targets so download links in the description don't appear to be a version (script 1843)
  s['versions'] = doc.search('a[@href*="download_script.php?"]').select { |e| e.attributes['target'].empty? }.to_a.map do |a|
    v = {'url' => 'https://www.vim.org/scripts/' + a.attributes['href'],
      'filename' => a.inner_content}
    row = a.parent
    v['script_version'] = row.siblings_at(1).inner_content
    v['date'] = row.siblings_at(2).inner_content
    v['vim_version'] = row.siblings_at(3).inner_content
    retryable(:on => retryable_errors) do |retries|
      v['author'] = scrape_author(row.siblings_at(4).at('a').attributes['href'].match(/\d+/)[0], retries)
    end
    v['release_notes'] = row.siblings_at(5).inner_content.gsub("\r", "\n")
    v
  end
  if s['versions'].empty?
    puts "Skipped #{script_id} -- empty."
    return nil
  end
  s
end


def fix_encoding(h)
  # see the Hpricot monkey patch above.  It gave us random encodings,
  # we need to force them back to their default before converting to utf8.
  # ugly!!
  if h.kind_of? Hash
    o = Hash.new
    h.each_pair { |k,v| o[fix_encoding(k)] = fix_encoding(v) }
    o
  elsif h.kind_of? Array
    a = Array.new
    h.each { |v| a.push fix_encoding(v) }
    a
  elsif h.kind_of? String
    if h.encoding.to_s == 'ASCII-8BIT' || h.encoding.to_s == 'ISO-8859-1'
      h = h.dup.force_encoding('ISO-8859-1').encode('UTF-8')
    end
    h
  else
    h
  end
end


def check_encoding(h)
  # recursively prints the encoding of every key/value/element etc
  if h.kind_of? Hash
    h.each_pair { |k,v| check_encoding(k); check_encoding(v) }
  elsif h.kind_of? Array
    h.each { |v| check_encoding(v) }
  elsif h.kind_of? String
    puts "#{h.encoding}: #{h[0..50]}"
  else
    h
  end
end


def h(*args)
  CGI.escapeHTML(*args)
end


def scripts_recent good_scripts
  good_scripts.map { |s|
    recent_author_name, recent_author_email = fix_email_address(s.versions.last.author)
    {
      :n => s.name,
      :t => s.script_type,
      :s => s.summary,
      :rv => s.versions.last.script_version,
      :rd => s.versions.last.date,
      :ra => recent_author_name,
      :re => recent_author_email
    }
  }.to_json
end


def generate_doc_files doc_dir
  puts "  reading scripts"

  # a hash of all scripts (including ones abandoned by renames) indexed by script_id
  repos = Dir.entries($repos_dir).reject { |e| %w{. .. .git}.include?(e) }
  all_scripts = repos.sort.map do |dir|
    Hashie::Mash.new(JSON.parse(File.read(File.join($repos_dir, dir, $git_script_file))))
  end
  return nil if all_scripts.empty?

  # just the official scripts -- no renames, no deletions
  script_files = Dir.entries($scripts_dir).reject { |e| %w{. .. .git}.include?(e) }
  good_scripts = script_files.sort.map do |file|
    Hashie::Mash.new(JSON.parse(File.read(File.join($scripts_dir, file))))
  end

  files = {
    'scripts.json'               => lambda { good_scripts.map { |s| s.name }.to_json },
    'script_ids.json'            => lambda { good_scripts.reduce({}) { |a,v| a[v.script_id.to_i] = v.name; a }.to_json },
    'script_original_names.json' => lambda { all_scripts.reduce({}) { |a,v| a[v.display_name] = v.name; a }.to_json },
    'scripts_recent.json'        => lambda { scripts_recent good_scripts }
  }

  files.each do |name,proc|
    puts "  generating #{doc_dir}/api/#{name}"
    File.open("#{doc_dir}/api/#{name}", 'w') do |f|
      f.write proc.call
    end
  end

  return files.keys.map { |name| "#{doc_dir}/api/#{name}" }
end


def generate_docs
  # wish we could use a bare repo to keep the docs but they don't support merging
  doc_dir = 'vim-scraper.github.com'
  puts "generating docs"

  site = nil
  unless test ?d, doc_dir
    site = GitRepo.new :root => doc_dir, :clone => "git@github.com:vim-scraper/vim-scraper.github.com.git"
    site.remote_add 'vim-scripts', "git@github.com:vim-scraper/vim-scripts.git"
  end

  site ||= GitRepo.new :root => doc_dir
  site.pull 'vim-scripts', 'master'

  updated_docs = generate_doc_files doc_dir
  if updated_docs
    author = { :name => $vimscripts_name, :email => $vimscripts_email }
    site.commit('new scrape', author) do |commit|
      updated_docs.each { |file| commit.add file, File.read(file) }
    end
  end

  site.push 'origin', 'master'
end


def filenameify(s)
  # replace unsafe path chars.  keep posessive Michaels, not Michael-s
  s.gsub(/^\s*|\s*$/, '').gsub(/'s/i, 's').gsub(/[^ A-Za-z0-9_\-!#\@\$^%&:;<?>+=(){|},.\[\]]/, '-')
end


def gittagify(s)
  # replace any chars git might take issue with (space, backslash, ^:)?]
  # git doesn't like it when a tag begins or ends with periods or dashes
  s.gsub(/^\s*|\s*$/, '').gsub(/[^A-Za-z0-9_\-!#\@\$%&;<>+=(){|},.\]]/, '-').gsub(/^\./, '0.').gsub(/\.$/, '.0').gsub(/^-*|-*$/, '').gsub(/^\./, '0.').gsub(/\.$/, '.0').gsub(/\.\./, '._')
end


def githubify(s)
  # these guys only allow A-Za-z0-9._- yet we need to try to keep the name readable.
  s.gsub(/^\s*|\s*$/, '').gsub(/\s+-|-\s+/, '-').gsub(/\+\+/, 'pp').gsub(/([CF])#/i, "#{$1}sharp").gsub('::', '.').gsub('&', 'and').gsub(/\s+|:|\+/, '-').gsub(/^-|-$/, '').gsub(/[^A-Za-z0-9_.-]/, '')
end


def script_version(version)
  # some scripts don't assign a version so just use the date
  s = version['script_version']
  s = version['date'] if s =~ /^\s*$/
  s
end


def hashkeyify name
  # converts a repo name into something worthy of matching against
  name.downcase
end


def script_filename(script)
  # if you change the filename format, also change script_extract_*
  File.join($scripts_dir, "#{'%04d' % script['script_id']} - #{filenameify(script['name'])}.json")
end


def script_extract_id script_name
  # ignore leading 0s, otherwise to_i might mistakenly think it's octal
  script_name =~ /^0*([0-9]+) - .+\.json$/ or raise "can't match #{script_name}"
  $1
end


def script_extract_name script_name
  script_name =~ /^[0-9]+ - (.+)\.json$/ or raise "can't match #{script_name}";
  $1
end


def repo_filename script_id, script_name
  File.join($repos_dir, "#{filenameify(script_name)}.git")
end


def repo_extract_name repo_name
  repo_name =~ /^(.+)\.git$/ or raise "can't match #{repo_name}";
  $1
end


def list_existing_scripts
  # returns a hash of key=script name, value=filename
  Hash[Dir.entries($repos_dir).reject { |e| %w{. .. .git}.include?(e) }.
    map { |e| [hashkeyify(repo_extract_name(e)), e]}]
end


def highest_script_id
  Dir.entries($scripts_dir).reject { |e| %w{. .. .git}.include?(e) }.
    map { |e| script_extract_id(e).to_i }.max
end


# if a script has been renamed, it will have multiple repos with
# the same id (the old ones will have a README pointing to the new one)
def find_scripts_by_id script_id
  Dir.entries($scripts_dir).reject { |e| %w{. .. .git}.include?(e) }.
    select { |e| script_extract_id(e).to_i == script_id.to_i }
end


# we don't try too hard to unobfuscate addresses but we definitely want them to be legal
# these rules were created by fiddling until most results looked plausible and all were legal.
def fix_email_address author
  email = author['email'].dup
  email = "unspecified@example.com" if email =~ /^\s*$/
  email.gsub!(/\s+[\[(]?at[)\]]?\s+/i, '@')
  email.gsub!(/\s+[\[(]?dot[)\]]?\s+/i, '.')
  # not sure how this next one will do with IDNs?
  # actually, without it we only fail on 9 of 1643 addresses
  # email.gsub!(/[^A-Za-z0-9!#\$%&'*+\/=?^`{|}~_@.-]/, '-')
  email.gsub!(/\s+|[:<>\[\]()"]/, '-')     # some common evil chars
  email.gsub!(/^\-+|\-+$/, '')             # no dashes at start or end
  email = "unspecified@example.com" if email =~ /^\s*$/
  email = "X#{email}" if email =~ /^@/     # fix "@gmail" with no local part
  email = "invalid@#{email}" unless email.include?('@')
  email = "#{email}.example.com" unless email =~ /[A-Za-z0-9]$/
  addr = Mail::Address.new(email) rescue Mail::Address.new("unparseable@example.com")
  addr.display_name = [author['first_name'], author['last_name']].select { |s| s =~ /\S/ }.join(' ').gsub(/\s+/, ' ')
  [addr.display_name, addr.address]
end


def fix_release_notes version
  msg = version['release_notes']
  msg.gsub!(/[ \t]+$/u, '')      # remove trailing whitespace on each line
  if msg.length > 70 || msg.include?("\n")
    # message is too long, we'll insert our own first line
    msg = "Version #{version['script_version']}\n\n#{msg}"
  else
    msg = "Version #{version['script_version']}: #{msg}"
  end
  msg + "\n"
end


def author script
  # returns the author's first and last name or nil if this script has multiple authors.
  # can't just check author id because a number of authors have abandoned old accounts and created new ones
  first_name = script['versions'][0]['author']['first_name']
  last_name  = script['versions'][0]['author']['last_name']
  if first_name =~ /^\s*$/ && last_name =~ /^\s*$/
    # user declined to state first and last name so we're forced to check by login
    last_name = script['versions'][0]['author']['user_name']
    script['versions'][1..-1].each { |v|
      return nil unless v['author']['user_name'] == last_name
    }
  else
    if last_name =~ /^\s*$/
      # if author states first name but not last name, we swap em.
      last_name = first_name
      first_name = ""
    end
    script['versions'][1..-1].each { |v|
      return nil unless v['author']['first_name'] == first_name && v['author']['last_name'] == last_name
    }
  end
  return [first_name, last_name]
end


def name_conflict_exists all_scripts, script
  # if the script's name doesn't conflict with any script in all_scripts,
  repo_dir = all_scripts[hashkeyify(script['name'])]
  return nil unless repo_dir    # no conflict

  # or if it's the same script, then that's OK.
  new_script = JSON.parse(File.read(File.join($repos_dir, repo_dir, $git_script_file)))
  return nil if new_script['script_id'].to_i == script['script_id'].to_i

  # there's a conflict.  return the conflicting script.
  new_script
end


# can't have two scripts with the same name on github.
# try to figure out an intelligent name for the newer repo.
def resolve_name_conflicts script
  return nil unless script
  all_scripts = list_existing_scripts

  existing_script = name_conflict_exists(all_scripts, script)
  return script unless existing_script

  # if the author is different, try that first
  script_author = author(script)
  if script_author && script_author != author(existing_script)
    script['display_name'] += ' -- ' + script_author[1]
    script['name'] += '--' + githubify(script_author[1])
  end
  existing_script = name_conflict_exists(all_scripts, script)
  return script unless existing_script

  # otherwise, see if we can differentiate by type
  if script['script_type'] != existing_script['script_type']
    script['display_name'] += ' ' + script['script_type']
    script['name'] += '-' + githubify(script['script_type'])
  end
  existing_script = name_conflict_exists(all_scripts, script)
  return script unless existing_script

  script['display_name'] += ' B'
  script['name'] += '-B'
  # otherwise, just tack a sequence letter on the end. didn't want to use a
  # number because "php.vim 2" looks like a newer release of "php.vim")
  while existing_script = name_conflict_exists(all_scripts, script)
    script['name'][-1] = (script['name'][-1].ord + 1).chr
    script['display_name'][-1] = (script['name'][-1].ord + 1).chr
    raise "what the heck?" if script['name'][-1] > 'Z'
  end

  script
end


def open_repo script_id, script_name
  repo_path = repo_filename script_id, script_name
  GitRepo.new(:root => repo_path, :bare => true, :create => true)
end


def mark_repo_as_duplicate dupe, new_script
  # add a commit that deletes all files and creates a README pointing to the new repo
  repo = open_repo(script_extract_id(dupe), script_extract_name(dupe))
  committer = { :name => $vimscripts_name, :email => $vimscripts_email }
  repo.commit("Renamed to #{new_script['display_name']}", committer, committer) do |commit|
    commit.empty_index
    commit.add 'README', "This script has been renamed to #{new_script['display_name']}.\n\n#{repo_url new_script}\n"
  end
end


# when a script gets renamed we copy the local repo so the git
# objects don't change and then install a README file in the old repo
def resolve_renamed_scripts script
  return nil unless script
  dupes = find_scripts_by_id(script['script_id']).reject do |x|
    # the new script isn't a duplicate
    script_extract_name(x) == script['name']
  end

  dupes.each do |dupe|
    puts "RENAMED: #{dupe} to #{script['display_name']}"

    new_repo = repo_filename(script['script_id'], script['name'])
    old_repo = repo_filename(script_extract_id(dupe), script_extract_name(dupe))
    # copy the existing repo so the objects don't change
    # (the new repo shouldn't exist but it's not worth dying if it does)
    FileUtils.cp_r old_repo, new_repo unless test ?d, new_repo

    # old repos have a bad timezone, use filter-branch to fix it before pushing
    # https://vim-scripts.org/news/2011/06/23/picky-about-timezones.html
    Dir.chdir(new_repo) do
      system "git filter-branch --env-filter '' --tag-name-filter cat HEAD"
      raise "git filter-branch failed: #{$?}" unless $?.success?
    end

    mark_repo_as_duplicate dupe, script
    puts "  pushing obsolete repo"
    perform_push old_repo
    # delete the obsolete script file
    File.delete File.join($scripts_dir, dupe)
  end

  script
end


def write_script script
  return unless script
  filename = script_filename(script)
  puts "Scraped #{filename}"
  File.open(filename, 'w') do |f|
    farg = fix_encoding(script)
    # check_encoding(farg)
    f.write json_pretty(farg)
  end
  filename
end


def compute_unique_tag tag, seen
  while seen[tag]
    # ack, it's a dupe!
    tag.sub! /@(\d+)$/, ''
    tag += "@" + (($1||0).to_i + 1).to_s
  end
  seen[tag] = true
  tag
end


def dedup_script_versions script
  # some scripts have versions with identical version numbers.  :(
  seen = {}
  script['versions'].reverse.each do |version|
    version['script_version'] = compute_unique_tag version['script_version'], seen
  end
end


def download_file url, dest
  retryable(:on => OpenURI::HTTPError, :task => "  downloading #{url} to #{dest}") do |retries|
    open(url, 'rb') do |u|
      File.open(dest, 'wb') { |f| f.write(u.read) }
    end
  end
end


def copy_file commit, filename, contents
  # skip swapfiles or crap Apple files that authors accidentally check in
  unless filename =~ /\.[^\/]+\.sw[n-p]$/ || filename =~ /~$/ || filename =~ /\.(?:_\.)?DS_Store$/ || filename =~ /(?:^|\/)\._/
    commit.add filename, contents
  end
end


def cleanpath path     # lifted from git-wiki
  path = path.gsub /^[\/\s]*/, ''
  names = path.split('/').reject { |str| str =~ /^\s*$/ }
  i = 0
  while i < names.length
    case names[i]
    when '..'
      names.delete_at(i)
      if i>0
        names.delete_at(i-1)
        i-=1
      end
    when '.'
      names.delete_at(i)
    else
      i+=1
    end
  end
  names.join('/')
end


# wish the site had a compiler file type.  as it is, we need to
# sniff the file contents to determine if it's a compiler plugin.
def is_compiler_file contents
  contents.lines.each do |l|
    next if l =~ /^\s*("|$)/;  # skip blank lines and comments
    # afaict 'if exists("current_compiler")' on the first line means compiler plugin
    return l =~ /^\s*if\s*\(?\s*exists\s*\(\s*["']current_compiler["']\s*\)/
  end
  return false
end


# sniff file contents to determine if it's a keymap file
def is_keymap_file contents
  # thanks to https://github.com/vim-scripts/greek_polytonic.vim, we can't assume
  # that we'll find keymap_name near the top of the file.
  # also see check_for_keymap_helper
  contents.lines.find { |line| line =~ /^\s*let\s+b:keymap_name\s*=/ }
end


# returns the new path if a change was made, or nil if not.
# returns true if the file should just be suppressed.
def fix_file_location script, path
  fix = $file_location_fixes[script['script_id'].to_i]
  if fix
    fix.each do |re,sub|
      if sub.nil?
        # suppress the file if re matches
        return path =~ re ? true : nil
      else
        # path substitution
        newpath = path.gsub re, sub
        return newpath if newpath != path
      end
    end
  end
  return nil
end


def smart_copy_file repo, script, filename, contents
  filename = cleanpath(filename)
  if newpath = fix_file_location(script, filename)
    copy_file repo, newpath, contents unless newpath == true
  elsif filename =~ /^[^\/]+\.vim$/
    # vimfile in the root directory
    encoded_contents = contents.dup    # this encoding stuff is killing me
    encoded_contents.force_encoding "ASCII-8BIT"

    if filename =~ /_options\.vim$/
      # convention seems to be to put example options to copy into
      # your vimrc in a file in the root dir called plugin_options.name
      copy_file(repo,  filename, contents)
    elsif is_compiler_file encoded_contents
      copy_file(repo, "compiler/" + filename, contents)
    elsif is_keymap_file encoded_contents
      copy_file(repo, "keymap/" + filename, contents)
    else
      case script['script_type']
      when 'color scheme' then copy_file(repo, "colors/" + filename, contents)
      when 'ftplugin' then copy_file(repo, "ftplugin/" + filename, contents)
      when 'game' then copy_file(repo, "plugin/" + filename, contents)
      when 'indent' then copy_file(repo, "indent/" + filename, contents)
      when 'syntax' then copy_file(repo, "syntax/" + filename, contents)
      when 'utility' then copy_file(repo, "plugin/" + filename, contents)
      when 'patch' then copy_file(repo, "plugin/" + filename, contents)
      else
        # if this fires, they must have added more script types?!
        raise "Don't know where to put #{filename} for #{script['script_type']}"
      end
    end
  elsif filename =~ /^[^\/]+\.txt$/
    # docfile in the root directory
    copy_file(repo, "doc/" + filename, contents)
  elsif filename =~ /(autoload|after)\/(#{$vimdirs.join('|')})\/([^\/]+)$/
    # vimdir in autoload or after: a/b/autoload/plugin/fixit.vim
    copy_file(repo, "#{$1}/#{$2}/#{$3}", contents)
  elsif filename =~ /^[^\/]+\/(#{$vimdirs.join('|')})\/([^\/]+)$/
    # developer put vimfiles in a subdir, i.e. fixit/plugin/fixit.vim.
    copy_file(repo, "#{$1}/#{$2}", contents)
  else
    copy_file(repo, filename, contents)
  end
end


def common_prefix set
  # https://stackoverflow.com/questions/1916218/find-the-longest-common-starting-substring-in-a-set-of-strings
  chars = set.map {|w| w.split('') }
  chars[0].zip(*chars[1..-1]).map { |c| c.uniq }.take_while { |c| c.size == 1 }.join
end


def copy_filesystem repo, script, dir, opts={}
  paths = []
  Find.find(dir) do |path|
    if test(?l, path) or path =~ /(?:^|\/)(\.git|.hg|\.bzr|\.svn)$/i
      Find.prune
    else
      # make sure all subdirs are readable
      File.chmod 0700, path if test ?d, path
      # only work on files, and ignore anything that the caller says should be skipped
      paths << path if test(?f, path) && (block_given? ? !yield(path) : true)
    end
  end

  return if paths.empty?    # script 1433 is all directories, no files!
  prefix = paths.count == 1 ?  File.dirname(paths.first) : common_prefix(paths)
  prefix.sub! /\/+[^\/]*$/, '' unless test ?d, prefix  # trim any partial filenames
  prefix.sub! /(?:^|\/)#{Regexp.union $vimdirs}(?:\/.*|$)/, ''  # don't trim any vim dirs

  paths.each do |path|
    # make all files rw and preserve the executable bit
    mode = File::Stat.new(path).mode
    File.chmod 0600 | (mode & 0700), path

    if opts[:smart]
      # trim as much as we can off the front of the path
      localpath = path.sub /^#{Regexp.escape prefix}\/*/, ''
    else
      # use the archive location as the path
      localpath = path.sub /^#{Regexp.escape dir}\/*/, ''
    end

    if opts[:smart]
      smart_copy_file repo, script, localpath, File.read(path)
    else
      copy_file repo, localpath, File.read(path)
    end
  end
end


def corrupt_vimball where
  puts "  corrupt vimball at #{where}"
  throw :corrupt
end


def unvimball repo, script, vimball
  # this routine deeply inspired by
  # https://github.com/carlhuda/janus/commit/6ae56116aa9f1cd65af43bd550405295ff2f5967
  lines = File.readlines(vimball).map &:chomp

  # If UseVimball and finish headers aren't found, archive is corrupt
  # This should be /^" Vimball Archiver/ but the BOM screws Ruby up.
  corrupt_vimball "A" unless lines.shift =~ /" Vimball Archiver/
  until (current = lines.shift) =~ /UseVimball\s*$/
    corrupt_vimball "B" unless current =~ /^\s*("|$)/
  end
  until (current = lines.shift) =~ /finish\s*$/
    corrupt_vimball "C" unless current =~ /^\s*("|$)/
  end

  while current = lines.shift
    # filename (followed by who knows), then the number of lines, then the data
    path = current
    path.sub! /\t\[\[\[1$/, ''
    path.gsub! '\\', '/'
    current = lines.shift
    num_lines = current[/^(\d+)$/, 1].to_i
    data = lines.slice!(0, num_lines)

    unless path == 'doc/tags'
      path = cleanpath path
      path = fix_file_location(script, path) || path
      copy_file repo, path, data.join("\n")+"\n" unless path == true
    end
  end
end


def unshell repo, script, localpath, cmd
  Dir.mktmpdir('scraper') do |tmpdir|
    fullpath = File.expand_path(localpath)
    Dir.chdir(tmpdir) do
      cmd = [*cmd, fullpath]
      # unzip returns a 1 exit code for success with warnings
      unless system(*cmd) || (cmd[0] == 'unzip' && $?.exitstatus == 1)
        raise "couldn't run #{cmd.join(' ')}: #{$?}"
      end
    end
    copy_filesystem repo, script, tmpdir, :smart => true
  end
end


def ungzip file
  Zlib::GzipReader.open(file) { |gz| yield(gz) }
end


def unbzip2 file
  Bzip2::Reader.open(file) { |bz| yield(bz) }
end


def unxz file
  IO.popen(['xz', '-d', '--to-stdout', file]) do |contents|
    yield(contents)
  end
end


def download_package version, script
  pkgname = "#{version['date']} #{filenameify(script_version(version))} #{filenameify(version['filename'])}"
  pkgdir = File.join($packages_dir, "#{'%04d' % script['script_id']} - #{filenameify(script['name'])}")
  Dir.mkdir pkgdir unless test ?d, pkgdir
  pkgfile = File.join(pkgdir, pkgname)
  download_file(version['url'], pkgfile) unless test ?f, pkgfile
  return pkgfile
end


def sense_zipped_file repo, script, filename, infile
  # sense the payload of a compressed file.  don't want to recurse into sense_file: too complex.
  contents = infile.read
  contents.force_encoding('ASCII-8BIT')
  Tempfile.open('scraper') do |ttfile|
    ttfile.write(contents)
    ttfile.close

    if MimeMagic.by_magic(contents) == 'application/x-tar'
      unshell repo, script, ttfile.path, ['tar', 'xf']
    elsif contents[0..512] =~ /^\bUseVimball\s*$/
      unvimball repo, script, ttfile.path
    else
      smart_copy_file repo, script, filename, contents
    end
  end
end


def is_some_sort_of_zipfile type
  type.include?('application/zip') || type.include?('application/x-gzip') || type.include?('application/x-bzip')
end


def sense_file repo, actual_name, pkgfile
  # some files lie about their type (claim to be .zips but are just .vim files).  fix em.
  extension_type = MIME::Types.type_for(actual_name)
  magic_type = File.open(pkgfile) { |f| MimeMagic.by_magic(f) }
  if ( extension_type.include?(magic_type) ||
    (extension_type.include?('application/x-bzip2') && magic_type == 'application/x-bzip') ||
    (extension_type.include?('application/x-rar-compressed') && magic_type == 'application/x-rar') )
    # we're good, extension and magic match up
  else
    # need to figure out what's going on and fix it.
    if (magic_type.nil? || magic_type.text?) && actual_name =~ /#{$textext}/
      # no problem, it's a textfile
    elsif actual_name =~ /\.vba$|\.vmb$|\.vimball$/
      # arg, a vimball.  let it through.  see sense_zipped_file for vimball sensing.
    elsif actual_name =~ /^[^a-z]*\.VIM$/
      # i guess dos users might do everything in caps
      actual_name.downcase!
    elsif actual_name =~ /^(.*)\.VIM$/
      actual_name = "#{$1}.vim"
    elsif ( %w{application/x-awk application/x-perl application/x-ruby application/x-shellscript application/xml}.include?(magic_type.to_s) ||
      (magic_type == nil && %w{vimopen cleanswap cvsvimdiff vmake vim_menu_HTMLpol}.include?(actual_name)) ||
      %w{zshrc vimrc _vimrc .vimrc}.include?(actual_name) ||
      actual_name =~ /\.dict$/ || %w{pydiction xdebug2}.include?(actual_name) ||  # dictionaries
      actual_name =~ /\.applescript$/ ||
      magic_type == 'application/x-java' ||
      magic_type == 'application/x-ms-dos-executable' )
      # names and magic have failed us, copy these files over raw
      copy_file(repo, actual_name, File.read(pkgfile))
      actual_name = nil
    elsif magic_type == 'application/x-tar' && extension_type.include?('application/x-gtar')
      # extension claims tarfile was gzipped but magic claims it wasn't.  Trust the magic.
      actual_name += '.tar'
    elsif magic_type == 'application/zip' && extension_type.include?('application/x-java-archive')
      # jarfiles sense as zipfiles so don't let the fixup code below "correct" the package extension
    elsif magic_type == 'application/zip'
      # authors have uploaded a lot of compressed archives without the correct extension.
      actual_name += '.zip'
    elsif magic_type == 'application/x-gzip'
      actual_name += '.gz'
    elsif magic_type == 'application/x-bzip'
      actual_name += '.bz2'
    elsif magic_type == 'application/x-7z-compressed'
      actual_name += '.7z'
    elsif magic_type == 'application/x-xz'
      actual_name += '.xz'
    elsif is_some_sort_of_zipfile(extension_type)
      # extension claims zipfile but magic disagrees, this happens a lot
      actual_name.sub!(/\.zip$|\.tar\.gz$|\.tar\.bz2?$|\.tgz$|\.tbz2?$/i, '')
      if magic_type == 'text/x-python'
        actual_name += '.py'
      elsif magic_type.nil? || magic_type.text?   # chances are it's a vimscript...?
        actual_name += '.vim'   # https://www.vim.org/scripts/script.php?script_id=29
      elsif magic_type == 'application/x-tar'
        actual_name += '.tar'
      else
        # just need to hope that nobody makes this mistake anymore.  if they do, fix this function.
        raise "unknown failed zip type for #{actual_name}: #{magic_type}"
      end
    elsif magic_type == 'application/x-macbinary'
      copy_file(repo, actual_name + '.macbinary', File.read(pkgfile))
      actual_name = nil
    elsif !magic_type && actual_name =~ /^exUtility-[0-9.]*.tar$/
      # odd that magic couldn't sense this valid tarfile.  owell, process it as normal.
    elsif extension_type.include?('application/x-tar') && magic_type == nil
      # magic has a bug where tarfiles are missed: https://github.com/minad/mimemagic/issues/#issue/1
      # when this happens, we just blindly trust the extension.
    else
      # there's such a range of reasons why this will happen that there's nothing to do but
      # have a human improve this function to handle this case.  :(
      raise "differing mime types for #{actual_name}, ext claims #{extension_type} but magic is #{magic_type.inspect}"
    end
  end
  return actual_name
end


def add_version repo, version, script
  # adds all the files in the package to the repo
  pkgfile = download_package(version, script)
  actual_name = sense_file(repo, version['filename'], pkgfile)

  case actual_name
  when nil then # do nothing

    # need to special-case 3584 - checklist.vim because the zipfile is too new.
    # TODO: switch to unzipping all zipfiles with 7z once this bug is fixed:
    # https://sourceforge.net/tracker/?func=detail&aid=3310980&group_id=14481&atid=114481
  when /checklist-v0.51.zip$/ then unshell repo, script, pkgfile, ['7z', 'x']

  when /#{$textext}/ then smart_copy_file repo, script, actual_name, File.read(pkgfile)
  when /\.zip$/i then unshell repo, script, pkgfile, ['unzip']
  when /\.tar$/ then unshell repo, script, pkgfile, ['tar', 'xf']
  when /\.t?gz$/i then ungzip(pkgfile) { |contents| sense_zipped_file(repo, script, version['filename'].sub(/\.t?gz$/, ''), contents) }
  when /\.t?bz2?$/i then unbzip2(pkgfile) { |contents| sense_zipped_file(repo, script, version['filename'].sub(/\.t?bz2?$/, ''), contents) }
  when /\.xz$/ then unxz(pkgfile) { |contents| sense_zipped_file(repo, script, version['filename'].sub(/\.xz$/, ''), contents) }
  when /\.7z$/ then unshell repo, script, pkgfile, ['7za', 'x']
  when /\.rar$/ then unshell repo, script, pkgfile, ['unrar', 'x']
  when /\.vba$|\.vmb$|\.vimball$/ then unvimball repo, script, pkgfile
  when /\.jar$/ then copy_file repo, actual_name, File.read(pkgfile)
  when /\.gif$|\.jpe?g$|\.png$/ then copy_file repo, actual_name, File.read(pkgfile)
  else
    # probably need to add a new text file extension to $textext.
    # if not, it's probably a new compression format.
    raise "unknown filetype: #{actual_name}"
  end
end


def check_for_readme_file commit
  # we drop a README file into each repo.  don't want to conflict with one already there.
  commit.entries.each do |name|
    if name =~ /^README$/i
      raise "already have a readme.orig!" if commit.entries.find { |n| n =~ /^readme\.orig$/i }
      commit.add name+'.orig', commit.remove(name)
    end
  end
end


def check_for_keymap_helper commit, script
  # It's easy to autodetect keymap files (see is_keymap_file above)
  # but some keymaps come with additional scripts that are not so common
  return unless commit.entry('keymap') && commit.entry('plugin')
  names = commit.entry('keymap', :tree)
  namesre = Regexp.union names
  commit.entry('plugin', :tree).each do |name|
    contents = commit.entry("plugin/#{name}").dup
    contents.force_encoding "ASCII-8BIT"
    if contents.lines.find { |line| line =~ /^\s*source\s+<sfile>:p:h\/#{namesre}/ }
      # yep, it's a keymap helper.
      data = commit.remove "plugin/#{name}"
      copy_file commit, "keymap/#{name}", data
    end
  end
end


def corrupted_package script, version
  # returns true if this version has been hard-coded as corrupted
  names = $skip_packages[script['script_id'].to_i]
  names && [names].flatten.include?("#{version['date']} #{version['script_version']}")
end


def store_versions_in_repo repo, script
  committer = { :name => $vimscripts_name, :email => $vimscripts_email }
  puts "Processing script #{script['script_id']}: #{script['name']}"
  count = 0

  script['versions'].reverse.each do |version|
    # todo: rather than bailing, we should bit-for-bit verify
    tagname = gittagify(script_version(version))
    next if repo.find_tag(tagname)

    next if corrupted_package(script, version)

    author_name, author_email = fix_email_address(version['author'])
    author = { :name => author_name, :email => author_email,
      :date => Time.new(*version['date'].split('-'), 0, 0, 0, 0) }

    catch :corrupt do
      puts "  adding #{version['filename']} #{version['date']} #{script_version(version)}"
      repo.commit(fix_release_notes(version), author, committer) do |commit|
        commit.empty_index
        add_version commit, version, script
        check_for_keymap_helper commit, script
        check_for_readme_file commit
        copy_file(commit, 'README', "This is a mirror of #{script_id_to_url(script['script_id'])}\n\n" + script['description'] + "\n") unless repo.root['README']
      end
      sver = script_version(version)
      puts "  tagging with #{gittagify(sver)} from #{sver}"
      repo.create_tag gittagify(sver), "tag #{sver}", committer
      count += 1
    end
  end
  count
end


def repo_url script
  "https://github.com/vim-scripts/#{script['name']}"
end


# push to vim-scripts.github.com so it we don't interfere with your regular ssh key.
# create a ~/.ssh/vimscripts-id_rsa and ~/.ssh/vimscripts-id_rsa.pub keypair,
# and create a ~/.ssh/config that has 2 Host sections:
#   Host github.com\nHostName github.com\nUser git\nIdentityFile ~/.ssh/id_rsa
#   Host vim-scripts.github.com\nHostName github.com\nUser git\nIdentityFile ~/.ssh/vimscripts-id_rsa
# see this for more: https://help.github.com/multiple-keys

def remote_url script
  "git@vim-scripts.github.com:vim-scripts/#{script['name']}"
end


def perform_push repo_name
  return unless repo_name
  github = GitHub.new
  repo = GitRepo.new :root => repo_name, :bare => true
  script = Hashie::Mash.new(JSON.parse(File.read(File.join(repo_name, $git_script_file))))
  puts "Uploading #{script['script_id']} - #{script['name']}"

  remote = begin
    github.info script.name
  rescue Octokit::NotFound
    nil
  end

  if remote
    # make sure this actually is the same repo
    puts "  remote already exists: #{remote.url}"
    remote.homepage =~ /script_id=(\d+)$/
    raise "bad url on github repo #{script['name']}" unless $1
    raise "remote #{script['name']} exists but id is for #{$1}" if script['script_id'] != $1
  else
    puts "  remote doesn't exist, creating..."
  end

  unless remote
    retryable(:task => "  creating #{script['script_id']} - #{script['name']}") do |retries|
      remote = github.create(script.name,
      :description => script.summary,
      :homepage => script_id_to_url(script.script_id),
      :public => true)
    end
  end

  repo.remote_remove 'origin' rescue nil
  repo.remote_add 'origin', remote_url(script)
  retryable(:task => "  #{"force " if ENV['FORCE']}pushing #{script['script_id']} - #{script['name']}") do |retries|
    args = ['--tags']
    args << '--force' if ENV['FORCE']
    args << 'origin'
    args << 'master'
    repo.push *args
  end
end


def perform_scrape script_id
  script = scrape_script script_id
  script = resolve_name_conflicts script
  script = resolve_renamed_scripts script
  write_script script
end


def perform_download script_file
  return [nil,-1] unless script_file
  script = JSON.parse(File.read(script_file))
  repo = open_repo script['script_id'], script['name']

  dedup_script_versions script

  # store the script file in the repo it creates
  File.open(File.join(repo.root, $git_script_file), 'w') { |f|
    f.write json_pretty(script)
  }

  count = store_versions_in_repo(repo, script)
  dump_repo "#{$repo_dir}/#{repo.root}", ENV['DUMP_REPO'] if ENV['DUMP_REPO']
  FileUtils.rm_rf "#{$repo_dir}/#{repo.root}" if ENV['PURGE_REPOS']
  [repo.root, count]
end


def dump_repo repo, stats_dir
  raise "no repo" unless repo
  repo = repo.gsub /\/$/, ''  # remove trailing / added by filename completion
  ENV['GIT_DIR'] = repo
  log = `git log --pretty=fuller --decorate=full --stat`
  log = log.each_line.reduce([]) { |a,v|
    v.gsub!(/^commit\s+[0-9A-Fa-f]*/, 'commit SHA1SHA1SHA1');  # make sha static, changes every run
    v.gsub!(/, refs\/(remotes|heads)[^,)]*\/master/, '');      # remove info about what's on the remote TODO: can I remove this?
    v.gsub!(/^( .*\|\s*[0-9]*)\s*[+-]*$/) { $1 };              # remove the git log --stat graph, it hides changes     sed -i 's/^\( .*\|\s*[0-9]*\)\s*[+-]*$/\1/' *
    a.push(v) unless v =~ /^CommitDate: /;                     # remove CommitDate since that's different each run
    a }.join
  ENV.delete('GIT_DIR')
  filename = File.join(stats_dir, File.basename(repo).gsub(/\.git$/, '') + '.stats')
  puts "Stats in #{filename}"
  File.open(filename, 'w') { |f|
    f.write log
  }
end


def perform_all script_id, always_push=false
  repo_name,count = perform_download(perform_scrape(script_id.to_s))
  if $pushing && (always_push or count > 0)
    perform_push repo_name
  end
  [repo_name, count]
end


def fetch_rss
  feed = Feedzirra::Feed.fetch_and_parse($rss_url)
  raise "No feed -- network disconnected?" if feed == 0
  puts "feed contains #{feed.entries.map { |e| script_id_from_url(e.url) }.inspect}"
  feed
end


def timeout_expired?
  if Time.now > $start_time + $max_run_time
    puts "We are now #{Time.now - ($start_time + $max_run_time)} seconds past time to quit."
    true
  else
    false
  end
end


def enter_full_mode state
  state.mode = 'full'
  feed = fetch_rss
  unless feed.entries.first
    puts "NO ENTRIES IN FEED??"
    exit 1
  end
  state.target_rss_id = feed.entries.first.entry_id
  state.full_index = 1
  puts "Entering full scrape mode!  Target is #{script_id_from_url feed.entries.first.url} / #{feed.entries.first.entry_id}."
end


def enter_rss_mode state
  state.last_rss_id = state.target_rss_id
  state.target_rss_id = nil
  state.full_index = nil
  state.mode = 'rss'
  puts "Entering RSS scrape mode!"
end


def perform_full
  state = read_state

  # The target rss id is the most recent rss id when we start the full scrape.
  # If it's still in the feed when we finish the full scrape, we're synced
  # and can return to rss mode.  If not, the full scrape must start again.
  if state.target_rss_id.nil? || state.mode != 'full'
    enter_full_mode state
  else
    puts "Resuming full scrape from #{state.full_index}.  Target is #{state.target_rss_id}."
  end

  highest_index = highest_script_id
  original_index = state.full_index
  while true
    break if timeout_expired?
    # Always push the first repo for the same reason as in perform_rss.
    # Only push subsequent repos if we actually downloaded changes because
    # pushing takes a lot of time, even if there's nothing to push.
    repo_name,count = perform_all(state.full_index, state.full_index == original_index)
    if repo_name.nil? && state.full_index > highest_index + 4
      # full scrape succeeded!
      enter_rss_mode state
      write_state state
      # do an rss scrape so we don't lose sync while waiting for the cronjob to fire
      perform_rss false
      generate_docs
      break
    end
    state.full_index += 1
    write_state state
  end
end


def perform_rss fallback=true
  state = read_state
  if state.mode != 'rss' || !state.last_rss_id
    perform_full
    return nil
  else
    puts "Resuming RSS scrape."
  end

  feed = fetch_rss
  entries = nil

  last_index = feed.entries.index { |e| e.entry_id == state.last_rss_id }
  if last_index
    if last_index == 0
      # rss is up to date!  nothing to scrape
      entries = []
    else
      entries = feed.entries[0..(last_index - 1)]
    end
  else
    puts "can't find  previous location #{state.last_rss_id} in the RSS feed."
    if fallback
      puts "Performing a full scrape from the beginning!"
      perform_full
    end
    entries = nil
  end

  if entries
    puts "   processing #{entries.map { |e| script_id_from_url(e.url) }.inspect}"
    entries.reverse.each do |entry|
      break if timeout_expired?
      script_id = script_id_from_url(entry.url)
      puts "Fetching script #{script_id} for rss #{entry.entry_id}"

      # always_push=true because there's a chance the previous scrape failed due to
      # a transient network error.  next time we run, we may find that there are
      # no versions to download, but the repo still needs to be pushed.
      perform_all(script_id, true)

      state.last_rss_id = entry.entry_id
      write_state state
    end
    generate_docs
  end

  entries ? entries.count : nil
end


def perform_continuous count
  state = read_state
  highest_script = highest_script_id

  # Since vim.org explodes its 10-item RSS fed and forces a rescrape every few weeks,
  # we'll never get past the first few hundred scripts if we always start from 0.
  # Starting from a random number ensures eventually we should visit every script.
  state.last_script_id ||= rand highest_script

  state.last_script_id += 1

  # keep looking past the last known id in case rss missed new scripts
  if state.last_script_id > highest_script + 3
    puts "Count is #{state.last_script_id} and the highest known script id is #{highest_script_id}.  Starting from 0!"
    state.last_script_id = 1
  end

  puts "Performing continuous scrape of #{state.last_script_id} through #{state.last_script_id+count-1}"
  limit = state.last_script_id + count
  while state.last_script_id < limit
    break if timeout_expired?
    puts "Fetching script #{state.last_script_id} for continuous scrape"
    perform_all state.last_script_id, true
    write_state state
    state.last_script_id += 1
  end
end


def fork_jobs args
  # This helps a lot (14 mins to 9 mins) but could even be better.
  # If 3 jobs are forked, 0-1100 gets done in 60% of the time of 2200-3300.
  # Might fork batches of 10 at a time or something...  not sure it's
  # worth the additional complexity just to save 30 sec though.
  #   todo: what about randomizing the argument order before grouping them?
  if ENV['JOBS']
    ENV['JOBS'].to_i.downto(2) do |i|
      workunit = args.count / i
      if i > 1 && workunit > 0 && fork.nil?
        args = args[0..workunit-1]
        $child_number = i
        break
      else # parent
        args = args[workunit..-1]
      end
    end
    puts "Processing #{args.size} unit#{'s' if args.size != 1}."
  end
  args
end


if __FILE__ == $0
  $start_time = Time.now

  if ARGV.empty?
    puts "Starting scrape at #{$start_time}"
    $ignore_cache = true
    # don't time out after 15 minutes if FOREVER=1
    $max_run_time = 10.days if ENV['FOREVER']
    count = perform_rss
    # also cycle through all packages in case the rss feed missed anything
    perform_continuous($idle_count - count) if count && $idle_count - count > 0
  elsif ARGV[0] == '--console'
    require 'irb'
    ARGV.shift
    IRB.start
  elsif ARGV[0] == '--docs'
    generate_docs
  elsif ARGV[0] == '--dump'
    args = ARGV[1..-1]
    stats_dir = 'scraper-stats'
    if args.empty?
      # this is a full dump so remove old dumpfiles
      Dir.entries(stats_dir).
        reject { |f| f == '.' || f == '..' || f == '.git' }.
        each { |f| File.delete(File.join(stats_dir, f)) }
      args = Dir.entries($repos_dir).
        reject { |f| f == '.' || f == '..' || f == '.git' }.
        map { |f| File.join $repos_dir, f }.
        sort
    end
    args.each { |repo| dump_repo repo, stats_dir }
  elsif ARGV[0] == '--all'
    # this generates all the repos but doesn't push them
    $max_run_time = 10.days
    $ignore_cache = false
    $pushing = false
    perform_full
  else
    args = fork_jobs ARGV
    args.each do |arg|
      # if you're passing args, you probably don't want the scraper to quit
      # before it's finished processing them all.
      $max_run_time = 10.days

      if arg =~ /^\d+$/
        $ignore_cache = true
        # leading zeros cause script ID to be interpreted as octal
        perform_all arg.gsub(/^0+/, ''), true
      elsif arg =~ /^-(\d+)$/
        # leading zeros cause script ID to be interpreted as octal
        perform_scrape $1.gsub(/^0+/, '')
      elsif test ?f, arg
        perform_download arg
      elsif test ?d, arg
        retryable(:task => "pushing #{arg}") do |retries|
          # sometimes, and it's hard to predict why, github errors out reliably
          # for a few minutes, then magically fixes itself.
          perform_push arg
        end
      elsif (f=Dir.glob("#{$scripts_dir}/???? - #{arg}.json")) && !f.empty? && test(?f, f.first)
        # plain script name -- must have already been scraped tho
        raise "multiple hits for #{arg}: #{f.inspect}" unless f.count == 1
        f.first =~ /^#{$scripts_dir}\/(\d\d\d\d)/
        perform_all $1.gsub(/^0+/, ''), true
      else
        raise "Could not recognize argument #{arg}"
      end
    end

    unless $child_number || args.count == ARGV.count
      puts "waiting for children to finish..."
      Process.wait
    end
  end

  stop = Time.now
  puts "done#{" #{$child_number}" if $child_number}.  Run took #{'%.2f' % (stop-$start_time)} seconds or #{ '%.3f' % ((stop-$start_time)/60)} minutes"

end

