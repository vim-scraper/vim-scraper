#!/usr/bin/env ruby

# vim scripts scraping monster
# This code is Copyright (c) 2010 Scott Bronson
# Released under the MIT License.
#
# DEPENDENCIES
# This script requires Ruby 1.9.2+.
# Make sure you have unzip, unrar, 7za, and xz installed.
#   Ubuntu: sudo apt-get install unzip unrar p7zip-full xz-utils
#   Macintosh: sudo port install unrar p7zip xz
#
# FULL SCRAPE:
# To start cold with a full scrape, do this:
#   rm state.json
#   FOREVER=1 ./scraper
# Now the scraper is warm and ready to perform continuous RSS scraping.
#
# AUTOMATIC RSS Scrape:
# Run the scraper with no args to perform an rss scrape.  It downloads
# new scripts in the rss feed and remembers its position for next time.
#     ./scraper
#
# MANUAL Debugging:
# - with a positive number, does a full scrape / upload cycle
#     ./scraper 987
# - with a negative number, scrapes but does not upload
#     ./scraper -987
# - with a .json file, creates the git repo
#     ./scraper scripts/0987*
# - with a bare git repo, pushes that repo up to github
#     ./scraper repos/0987*
#
# TESTING:
# You may think that this script has no tests.  Not true!  The run-test
# script creates a repo for every script in the system, runs git log on
# it, and dumps the results.  If anything has changed, you'll see it.
#   JOBS=4 ./run-test stock
#   cd result/stats-stock
#   git status; git diff; etc.
# JOBS (optional): splits args into N groups and runs them in parallel.
#
# Because authors are always renaming scripts, changing email addrs and
# readmes, and deleting revisions, a from-scratch scrape will always
# be different from the repos in github.  Therefore, you should do this
# after every few scrapes:
#   rm scraper-stats/*
#   ./scraper --dump repos/*
#
# In the scraper-stats repo, the master branch contains the ongoing repos
# and the test branch contains the recreate-from-scratch each time repos.
#
# RESCUING:
# If you deleted a script repo and want to restore your local copy
# with the one on github:
#    git clone --bare git://github.com/vim-scripts/SCRIPTNAME
#    ./scraper SCRIPTID      # to restore internal state
# Now the script is exactly the same as if it had been scraped
# locally from the start.


require 'rubygems'
require 'bundler'
Bundler.require

require 'hpricot'             # hpricot gem
require 'open-uri'
require 'cgi'
require 'json'                # json gem
require 'gitrb'               # gitrb gem
require 'zlib'
require 'bzip2'               # bzip2-ruby gem
require 'mime/types'
require 'mimemagic'           # mimemagic gem
require 'tmpdir'
require 'tempfile'
require 'find'
require 'octopussy'           # octopussy gem
require 'hashie'              # hashie gem
require 'htmlentities'        # htmlentities gem
require 'feedzirra'           # feedzirra gem
require 'erubis'              # erubis gem
require 'mail'                # mail gem
require 'fileutils'
require 'selenium/client'     # selenium-client gem
require 'open3'


# This is the name and email address of the git committer.
$vimscripts_name = "Able Scraper"
$vimscripts_email = 'scraper@vim-scripts.org'

$rss_url = 'http://feed43.com/vim-scripts.xml'
$idle_count = 10    # number of scripts to scrape when we have nothing else to do
$max_run_time = 14.minutes + 30.seconds    # script exits normally once this time has elapsed
$repos_dir = ENV['REPOS_DIR'] || 'repos'        # if a repo has been renamed there will be duplicate ids in here
$scripts_dir = 'scripts'    # should never be any duplicate ids in here
$packages_dir = 'packages'
$git_script_file = 'vim-script.json'   # the file in the bare repo that stores the script that generated it
$state_file = 'state.json'             # keeps track of what mode we're in (full vs. rss scrape)
$pushing = true             # set this to false to prevent pushing (normally, if we find changes in a repo, we push)

$vimdirs = %w{after autoload bin compiler colors doc ftdetect ftplugin indent keymap plugin syntax}
$textext = %w{au3 bat c cpp csh diff h klip patch pl pm ps py rb set sh snip snippet snippets tcl txt xml vim vim.orig}.map { |x| "\\.#{x}$" }.join('|')

# vim.org doesn't offer any way to delete a script so people have invented all sorts of ways of doing it.
# 1022 2668 2730, and 3080 have also been deleted but I give up.  This is good enough.
$deleted_scripts = %w{364 548 549 550 762 1032 1129 1263 1280 1295 1301 1430 1436 1452 1509 1562 1669 1789 1824 1949 2056 2172 2306 2309 2313 2316 2318 2323 2352 2456 2498 2664 2861 3076 3080}

# not sure why an author would leave a corrupt package on vim scripts forever but owell.
# also we can't trust the script version and date pair to be unique: script 2709 SudoEdit.vim
$skip_packages = {
    1609 =>  ["2006-10-06 3.8", "2006-10-13 3.9", "2006-11-07 4.1.0", "2006-11-08 4.2.0", "2006-11-18 4.3.0"],
    3075 => ["2010-07-28 0.12", "2010-07-28 0.11"],
}

# some scripts were created with the wrong type.  this fixes the ones that matter.
#     http://groups.google.com/group/vim_use/msg/6f9f82e8c6fb4faa
# note that you must re-scrape after adding a fix ("./scraper 1780").
# regenerating ("./scraper scripts/1780*") is not sufficient in this case.
$script_type_fixes = {
    93   => 'ftplugin',
    1780 => 'syntax',
}

# if the regex matches the path, it is run through gsub and the result
# used as the new path.  If the replace string is nil, the file is
# suppressed.   Doesn't work for gifs and a few others (easy to fix).
$file_location_fixes = {
    284  => {      /([^\/]+\.vim)$/ => 'ftplugin/tex/\1' }, # all .vim files go in ftplugin/tex
    1095 => { /^.*\/([^\/]+\.vim)$/ => 'ftplugin/tex/\1' }, # all .vim files go in ftplugin/tex
    1771 => { /readme$/ => nil                           }, # has identical README and readme files.
    2651 => { /^(.*)Syntax(.*)$/ => '\1syntax\2'         }, # breaks on case sensitive filesystems
    3027 => { /^root\/\.vim\/(.*)$/ => '\1'              }, # grsecurity balled up his entire root dir
}

# at least one script is actually multiple scripts -- 790 has python.vim and python3.0.vim
# this forces package names matching the regex out to a different branch
$branch_versions = { 790 => { :branch => 'python3', :regex => /^python3\.0\.vim$/ } }


Dir.mkdir $scripts_dir unless test ?d, $scripts_dir
Dir.mkdir $repos_dir unless test ?d, $repos_dir
Dir.mkdir $packages_dir unless test ?d, $packages_dir

# this mime magic is far too vague.  it false-triggers all the time.
MimeMagic.remove 'application/x-gmc-link'


# http://github.com/hpricot/hpricot/issues#issue/25
# super ugly that hpricot manages to screw up charset encodings so badly.

# hopefully the next version of hpricot allows us to get rid of these monkeypatches
module Hpricot
    module Traverse
        def inner_text
            if respond_to?(:children) and children
#                children.map { |x| x.inner_text }.join
                children.map { |x| str = x.inner_text;
                    str = str.dup.force_encoding('ISO-8859-1').encode('UTF-8') if str.encoding.to_s == 'ASCII-8BIT' || str.encoding.to_s == 'ISO-8859-1';
                    str
                }.join
            else
                ""
            end
        end
    end

    def self.uxs(str)
        str = str.to_s
        str = str.dup.force_encoding('ISO-8859-1').encode('UTF-8') if str.encoding.to_s == 'ASCII-8BIT' || str.encoding.to_s == 'ISO-8859-1'
        str.gsub(/\&(\w+);/) { [NamedCharacters[$1] || 63].pack("U*") }.
            gsub(/\&\#(\d+);/) { [$1.to_i].pack("U*") }
    end
end

# and and hopefully a future version will allow us to remove these monkeypatches
module Hpricot
    module Traverse
        def inner_content
            if respond_to?(:children) and children
                children.map { |x| x.inner_content }.join
            else
                ''
            end
        end
    end

    class Elements < Array
        def inner_content
            map { |x| x.inner_content }.join
        end
    end

    class Text
        def inner_content
            str = content.to_s
            str = str.force_encoding('ISO-8859-1').encode("UTF-8") if str.encoding.to_s == 'ASCII-8BIT'
            CGI.unescapeHTML(HTMLEntities.new.decode(str.gsub(/&nbsp;/, " ")))
        end
    end

    class CData
        alias_method :inner_content, :content
    end
end


# http://blog.codefront.net/2008/01/14/retrying-code-blocks-in-ruby-on-exceptions-whatever/
# modified to pass the number of retries in the arg (0,1,2 if :tries => 3)
# todo: should use exponential backoff and also pass the exception to the block
def retryable(options = {}, &block)
    opts = { :tries => 4, :on => Exception, :sleep => 10 }.merge(options)
    return if opts[:tries] < 1
    tries = 0

    prev_exception = nil
    if opts[:tries] > 1
        begin
            return yield tries, prev_exception
        rescue Exception => e
            prev_exception = e
            sleep opts[:sleep]
            retry if (tries += 1) < opts[:tries] - 1
        end
    end

    # last try will throw an exception
    yield tries, prev_exception
end

# When running normally, we can be in one of two states: rss and full.
# rss is when the rss is synced and we can update only the scripts that have changed.
# full is when we're just starting or we have lost the rss sync.

def read_state
    json = JSON.parse(File.read($state_file)) rescue nil
    Hashie::Mash.new json
end


def write_state state
    File.open($state_file, 'w') { |f| f.write(JSON.pretty_generate(state)+"\n") }
end


def scrape_author(user_id)
    $authors ||= []
    unless $authors[user_id.to_i]
        doc = retryable { open("http://www.vim.org/account/profile.php?user_id=#{user_id}") { |f| Hpricot(f) } }
        # How strange.  If the next line throws an "uknown next_sibling method for nil" error, vim-scripts
        # probably is suffering a temporary error.  Try the query again and it should succeed.  :-/
        doc.at('td[text()="user name"]').next_sibling.inner_content
        u = {'user_id' => user_id }
        u['user_name'] = doc.at('td[text()="user name"]').next_sibling.inner_content
        u['first_name'] = doc.at('td[text()="first name"]').next_sibling.inner_content
        u['last_name'] = doc.at('td[text()="last name"]').next_sibling.inner_content
        u['email'] = doc.at('td[text()="email"]').next_sibling.inner_content
        u['homepage'] = doc.at('td[text()="homepage"]').next_sibling.inner_content
        $authors[user_id.to_i] = u
    end
    return $authors[user_id.to_i]
end


def script_id_to_url(script_id)
    "http://www.vim.org/scripts/script.php?script_id=#{script_id}"
end


def script_id_from_url(url)
    url =~ /[?&;]script_id=(\d+)/ or raise "Could not parse a script id from <<#{url}>>"
    $1
end


def scrape_script(script_id)
    script_id = script_id.to_s

    if $deleted_scripts.include? script_id
        puts "Skipped #{script_id} -- deleted."
        return nil
    end

    doc = retryable { open(script_id_to_url(script_id)) { |f| Hpricot(f) } }
    if doc.search('title').inner_text == "Error : vim online"
        puts "Skipped #{script_id} -- doesn't exist."
        return nil
    end

    s = {'script_id' => script_id}
    s['display_name'], s['summary'] = doc.search('.txth1').inner_content.split(" : ", 2)
    s['name'] = githubify(s['display_name'])
    s['script_type'] = $script_type_fixes[script_id.to_i] ||
      doc.at('td[text()="script type"]').parent.next_sibling.children.first.inner_content

    desc = doc.at('td[text()="description"]').parent.next_sibling.children.first
    desc.search('br').each do |br|
        # restore the newline to every element preceeding a br.
        prev = br.previous;
        if prev && prev.text?
            prev.content = prev.content + "\r" unless prev.content.end_with?("\r")
        else
            br.before "\r"
        end
    end
    s['description'] = desc.inner_content.gsub("\r", "\n")

    if false     # we don't use this info and it generates too much noise
        doc.search('td.lightbg~td').find { |e| e.inner_text =~ /Rating.*\s(-?\d+)\/(\d+),.*Downloaded[^\d]*(\d+)/m }
        s['rating_total'], s['rating_votes'], s['downloads'] = $1, $2, $3      # http://www.vim.org/karma.php
    end

    s['install_details'] = doc.at('td[text()="install details"]').parent.next_sibling.children.first.inner_content.gsub("\r", "\n")
    # reject links with targets so download links in the description don't appear to be a version (script 1843)
    s['versions'] = doc.search('a[@href*="download_script.php?"]').select { |e| e.attributes['target'].empty? }.to_a.map do |a|
        v = {'url' => 'http://www.vim.org/scripts/' + a.attributes['href'],
            'filename' => a.inner_content}
        row = a.parent
        v['script_version'] = row.siblings_at(1).inner_content
        v['date'] = row.siblings_at(2).inner_content
        v['vim_version'] = row.siblings_at(3).inner_content
        v['author'] = scrape_author(row.siblings_at(4).at('a').attributes['href'].match(/\d+/)[0])
        v['release_notes'] = row.siblings_at(5).inner_content.gsub("\r", "\n")
        v
    end
    if s['versions'].empty?
        puts "Skipped #{script_id} -- empty."
        return nil
    end
    s
end


def fix_encoding(h)
    # see the Hpricot monkey patch above.  It gave us random encodings,
    # we need to force them back to their default before converting to utf8.
    # ugly!!
    if h.kind_of? Hash
        o = Hash.new
        h.each_pair { |k,v| o[fix_encoding(k)] = fix_encoding(v) }
        o
    elsif h.kind_of? Array
        a = Array.new
        h.each { |v| a.push fix_encoding(v) }
        a
    elsif h.kind_of? String
        if h.encoding.to_s == 'ASCII-8BIT' || h.encoding.to_s == 'ISO-8859-1'
            h = h.dup.force_encoding('ISO-8859-1').encode('UTF-8')
        end
        h
    else
        h
    end
end


def check_encoding(h)
    # recursively prints the encoding of every key/value/element etc
    if h.kind_of? Hash
        h.each_pair { |k,v| check_encoding(k); check_encoding(v) }
    elsif h.kind_of? Array
        h.each { |v| check_encoding(v) }
    elsif h.kind_of? String
        puts "#{h.encoding}: #{h[0..50]}"
    else
        h
    end
end


def h(*args)
    CGI.escapeHTML(*args)
end


def generate_docs
    # a hash of all scripts (including ones abandoned by renames) indexed by script_id
    repos = Dir.entries($repos_dir).reject { |e| %w{. .. .git}.include?(e) }
    all_scripts = repos.sort.map do |dir|
        Hashie::Mash.new(JSON.parse(File.read(File.join($repos_dir, dir, $git_script_file))))
    end

    # just the official scripts -- no renames, no deletions
    script_files = Dir.entries($scripts_dir).reject { |e| %w{. .. .git}.include?(e) }
    good_scripts = script_files.sort.map do |file|
        Hashie::Mash.new(JSON.parse(File.read(File.join($scripts_dir, file))))
    end

    doc_dir = 'vim-scripts.github.com'
    Dir.mkdir doc_dir unless test ?d, doc_dir

    index_template = Erubis::Eruby.new(File.new(File.join('views', 'scripts.html.erb')).read)
    File.open("#{doc_dir}/vim/scripts.html", 'w') do |f|
        f.write index_template.result(:scripts => good_scripts)
    end


    File.open("#{doc_dir}/api/scripts.json", 'w') do |f|
        f.write good_scripts.map { |s| s.name }
    end

    File.open("#{doc_dir}/api/scripts_recent.json", 'w') do |f|
        f.write good_scripts.map { |s|
            recent_author_name, recent_author_email = fix_email_address(s.versions.last.author)
            {
                :n => s.name,
                :s => s.summary,
                :rv => s.versions.last.script_version,
                :rd => s.versions.last.date,
                :ra => recent_author_name,
                :re => recent_author_email
            }
        }
    end

    # each script id and its official, current name.
    File.open("#{doc_dir}/api/script_ids.json", 'w') do |f|
        f.write good_scripts.reduce({}) { |a,v| a[v.script_id.to_i] = v.name; a }.to_json
    end

    # original display name for scripts, and their IDs.
    File.open("#{doc_dir}/api/script_original_names.json", 'w') do |f|
        f.write all_scripts.reduce({}) { |a,v| a[v.display_name] = v.name; a }.to_json
    end
end


def filenameify(s)
    # replace unsafe path chars.  keep posessive Michaels, not Michael-s
    s.gsub(/^\s*|\s*$/, '').gsub(/'s/i, 's').gsub(/[^ A-Za-z0-9_\-!#\@\$^%&:;<?>+=(){|},.\[\]]/, '-')
end


def gittagify(s)
    # replace any chars git might take issue with (space, backslash, ^:)?]
    # git doesn't like it when a tag begins or ends with periods or dashes
    s.gsub(/^\s*|\s*$/, '').gsub(/[^A-Za-z0-9_\-!#\@\$%&;<>+=(){|},.\]]/, '-').gsub(/^\./, '0.').gsub(/\.$/, '.0').gsub(/^-*|-*$/, '').gsub(/^\./, '0.').gsub(/\.$/, '.0')
end


def githubify(s)
    # these guys only allow A-Za-z0-9._- yet we need to try to keep the name readable.
    s.gsub(/^\s*|\s*$/, '').gsub(/\s+-|-\s+/, '-').gsub(/\+\+/, 'pp').gsub(/([CF])#/i, "#{$1}sharp").gsub('::', '.').gsub('&', 'and').gsub(/\s+|:|\+/, '-').gsub(/^-|-$/, '').gsub(/[^A-Za-z0-9_.-]/, '')
end


def script_version(version)
    # some scripts don't assign a version so just use the date
    s = version['script_version']
    s = version['date'] if s =~ /^\s*$/
    s
end


def hashkeyify name
    # converts a repo name into something worthy of matching against
    name.downcase
end


def script_filename(script)
    # if you change the filename format, also change script_extract_*
    File.join($scripts_dir, "#{'%04d' % script['script_id']} - #{filenameify(script['name'])}.json")
end


def script_extract_id script_name
    # ignore leading 0s, otherwise to_i might mistakenly think it's octal
    script_name =~ /^0*([0-9]+) - .+\.json$/ or raise "can't match #{script_name}"
    $1
end


def script_extract_name script_name
    script_name =~ /^[0-9]+ - (.+)\.json$/ or raise "can't match #{script_name}";
    $1
end


def repo_filename script_id, script_name
    File.join($repos_dir, "#{filenameify(script_name)}.git")
end


def repo_extract_name repo_name
    repo_name =~ /^(.+)\.git$/ or raise "can't match #{repo_name}";
    $1
end


def list_existing_scripts
    # returns a hash of key=script name, value=filename
    Hash[Dir.entries($repos_dir).reject { |e| %w{. .. .git}.include?(e) }.
        map { |e| [hashkeyify(repo_extract_name(e)), e]}]
end


def highest_script_id
    Dir.entries($scripts_dir).reject { |e| %w{. .. .git}.include?(e) }.
        map { |e| script_extract_id(e).to_i }.max
end


# if a script has been renamed, it will have multiple repos with
# the same id (the old ones will have a README pointing to the new one)
def find_scripts_by_id script_id
    Dir.entries($scripts_dir).reject { |e| %w{. .. .git}.include?(e) }.
        select { |e| script_extract_id(e).to_i == script_id.to_i }
end


# we don't try too hard to unobfuscate addresses but we definitely want them to be legal
# these rules were created by fiddling until most results looked plausible and all were legal.
def fix_email_address author
    email = author['email'].dup
    email = "unspecified@example.com" if email =~ /^\s*$/
    email.gsub!(/\s+[\[(]?at[)\]]?\s+/i, '@')
    email.gsub!(/\s+[\[(]?dot[)\]]?\s+/i, '.')
    # not sure how this next one will do with IDNs?
    # actually, without it we only fail on 9 of 1643 addresses
    # email.gsub!(/[^A-Za-z0-9!#\$%&'*+\/=?^`{|}~_@.-]/, '-')
    email.gsub!(/\s+|[:<>\[\]()"]/, '-')     # some common evil chars
    email.gsub!(/^\-+|\-+$/, '')             # no dashes at start or end
    email = "unspecified@example.com" if email =~ /^\s*$/
    email = "X#{email}" if email =~ /^@/     # fix "@gmail" with no local part
    email = "invalid@#{email}" unless email.include?('@')
    email = "#{email}.example.com" unless email =~ /[A-Za-z0-9]$/
    addr = Mail::Address.new(email) rescue Mail::Address.new("unparseable@example.com")
    addr.display_name = [author['first_name'], author['last_name']].select { |s| s =~ /\S/ }.join(' ').gsub(/\s+/, ' ')
    [addr.display_name, addr.address]
end


def fix_release_notes version
    msg = version['release_notes']
    msg.gsub!(/[ \t]+$/u, '')      # remove trailing whitespace on each line
    if msg.length > 70 || msg.include?("\n")
        # message is too long, we'll insert our own first line
        msg = "Version #{version['script_version']}\n\n#{msg}"
    else
        msg = "Version #{version['script_version']}: #{msg}"
    end
    msg + "\n"
end


def author script
    # returns the author's first and last name or nil if this script has multiple authors.
    # can't just check author id because a number of authors have abandoned old accounts and created new ones
    first_name = script['versions'][0]['author']['first_name']
    last_name  = script['versions'][0]['author']['last_name']
    if first_name =~ /^\s*$/ && last_name =~ /^\s*$/
        # user declined to state first and last name so we're forced to check by login
        last_name = script['versions'][0]['author']['user_name']
        script['versions'][1..-1].each { |v|
            return nil unless v['author']['user_name'] == last_name
        }
    else
        if last_name =~ /^\s*$/
            # if author states first name but not last name, we swap em.
            last_name = first_name
            first_name = ""
        end
        script['versions'][1..-1].each { |v|
            return nil unless v['author']['first_name'] == first_name && v['author']['last_name'] == last_name
        }
    end
    return [first_name, last_name]
end


def name_conflict_exists all_scripts, script
    # if the script's name doesn't conflict with any script in all_scripts,
    repo_dir = all_scripts[hashkeyify(script['name'])]
    return nil unless repo_dir    # no conflict

    # or if it's the same script, then that's OK.
    new_script = JSON.parse(File.read(File.join($repos_dir, repo_dir, $git_script_file)))
    return nil if new_script['script_id'].to_i == script['script_id'].to_i

    # there's a conflict.  return the conflicting script.
    new_script
end


# can't have two scripts with the same name on github.
# try to figure out an intelligent name for the newer repo.
def resolve_name_conflicts script
    return nil unless script
    all_scripts = list_existing_scripts

    existing_script = name_conflict_exists(all_scripts, script)
    return script unless existing_script

    # if the author is different, try that first
    script_author = author(script)
    if script_author && script_author != author(existing_script)
        script['display_name'] += ' -- ' + script_author[1]
        script['name'] += '--' + githubify(script_author[1])
    end
    existing_script = name_conflict_exists(all_scripts, script)
    return script unless existing_script

    # otherwise, see if we can differentiate by type
    if script['script_type'] != existing_script['script_type']
        script['display_name'] += ' ' + script['script_type']
        script['name'] += '-' + githubify(script['script_type'])
    end
    existing_script = name_conflict_exists(all_scripts, script)
    return script unless existing_script

    script['display_name'] += ' B'
    script['name'] += '-B'
    # otherwise, just tack a sequence letter on the end. didn't want to use a
    # number because "php.vim 2" looks like a newer release of "php.vim")
    while existing_script = name_conflict_exists(all_scripts, script)
        script['name'][-1] = (script['name'][-1].ord + 1).chr
        script['display_name'][-1] = (script['name'][-1].ord + 1).chr
        raise "what the heck?" if script['name'][-1] > 'Z'
    end

    script
end


def open_repo script_id, script_name
    repo_path = repo_filename script_id, script_name
    # if gitrb is dying on the following line, you need to upgrade
    Gitrb::Repository.new(:path => repo_path, :bare => true, :create => true)
end


def mark_repo_as_duplicate dupe, new_script
    # add a commit that deletes all files and creates a README pointing to the new repo
    repo = open_repo(script_extract_id(dupe), script_extract_name(dupe))
    committer = Gitrb::User.new($vimscripts_name, $vimscripts_email)
    repo.transaction("Renamed to #{new_script['display_name']}", committer, committer) do
        repo.root.to_a.map { |name,value| repo.root.delete(name) }
        repo.root['README'] = Gitrb::Blob.new(:data => "This script has been renamed " +
          "to #{new_script['display_name']}.\n\n#{repo_url new_script}\n")
    end
end


# when a script gets renamed we copy the local repo so the git
# objects don't change and then install a README file in the old repo
def resolve_renamed_scripts script
    return nil unless script
    dupes = find_scripts_by_id(script['script_id']).reject do |x|
        # the new script isn't a duplicate
        script_extract_name(x) == script['name']
    end

    dupes.each do |dupe|
        puts "RENAMED: #{dupe} to #{script['display_name']}"

        new_repo = repo_filename(script['script_id'], script['name'])
        old_repo = repo_filename(script_extract_id(dupe), script_extract_name(dupe))
        # copy the existing repo so the objects don't change
        # (the new repo shouldn't exist but it's not worth dying if it does)
        FileUtils.cp_r old_repo, new_repo unless test ?d, new_repo

        mark_repo_as_duplicate dupe, script
        # delete the obsolete script file
        File.delete File.join($scripts_dir, dupe)
    end

    script
end


def write_script script
    return unless script
    filename = script_filename(script)
    puts "Scraped #{filename}"
    File.open(filename, 'w') do |f|
        farg = fix_encoding(script)
        # check_encoding(farg)
        f.write(JSON.pretty_generate(farg)+"\n")
    end
    filename
end


def compute_unique_tag tag, seen
    while seen[tag]
        # ack, it's a dupe!
        tag.sub! /@(\d+)$/, ''
        tag += "@" + (($1||0).to_i + 1).to_s
    end
    seen[tag] = true
    tag
end


def dedup_script_versions script
    # some scripts have versions with identical version numbers.  :(
    seen = {}
    script['versions'].reverse.each do |version|
        version['script_version'] = compute_unique_tag version['script_version'], seen
    end
end


def download_file url, dest
    retryable(:tries => 4, :sleep => 10) do |retries|
        puts "  downloading #{url} to #{dest}#{retries > 0 ? "  TRY #{retries}" : ""}"
        open(url, 'rb') do |u|
            File.open(dest, 'wb') { |f| f.write(u.read) }
        end
    end
end


def copy_file repo, filename, contents
    # an empty file is represented by the empty string.  contents==nil indicates an internal error.
    raise "no data in #{filename}: #{contents.inspect}" unless contents

    # skip swapfiles or crap Apple files that authors accidentally check in
    unless filename =~ /\.[^\/]+\.sw[n-p]$/ || filename =~ /~$/ || filename =~ /\.(?:_\.)?DS_Store$/ || filename =~ /(?:^|\/)\._/
        repo.root[filename] = Gitrb::Blob.new(:data => contents)
    end
end


def cleanpath path     # lifted from git-wiki
    path = path.gsub /^[\/\s]*/, ''
    names = path.split('/').reject { |str| str =~ /^\s*$/ }
    i = 0
    while i < names.length
        case names[i]
        when '..'
            names.delete_at(i)
            if i>0
                names.delete_at(i-1)
                i-=1
            end
        when '.'
            names.delete_at(i)
        else
            i+=1
        end
    end
    names.join('/')
end


# wish the site had a compiler file type.  as it is, we need to
# sniff the file contents to determine if it's a compiler plugin.
def is_compiler_file contents
    contents.lines.each do |l|
        next if l =~ /^\s*("|$)/;  # skip blank lines and comments
        # afaict 'if exists("current_compiler")' on the first line means compiler plugin
        return l =~ /^\s*if\s*\(?\s*exists\s*\(\s*["']current_compiler["']\s*\)/
    end
    return false
end


# sniff file contents to determine if it's a keymap file
def is_keymap_file contents
    # thanks to http://github.com/vim-scripts/greek_polytonic.vim, we can't assume
    # that we'll find keymap_name near the top of the file.
    # also see check_for_keymap_helper
    contents.lines.find { |line| line =~ /^\s*let\s+b:keymap_name\s*=/ }
end


# returns the new path if a change was made, or nil if not.
# returns true if the file should just be suppressed.
def fix_file_location script, path
    fix = $file_location_fixes[script['script_id'].to_i]
    if fix
        fix.each do |re,sub|
            if sub.nil?
                # suppress the file if re matches
                return path =~ re ? true : nil
            else
                # path substitution
                newpath = path.gsub re, sub
                return newpath if newpath != path
            end
        end
    end
    return nil
end


def smart_copy_file repo, script, filename, contents
    filename = cleanpath(filename)
    if newpath = fix_file_location(script, filename)
        copy_file repo, newpath, contents unless newpath == true
    elsif filename =~ /^[^\/]+\.vim$/
        # vimfile in the root directory
        encoded_contents = contents.dup    # this encoding stuff is killing me
        encoded_contents.force_encoding "ASCII-8BIT"

        if filename =~ /_options\.vim$/
            # convention seems to be to put example options to copy into
            # your vimrc in a file in the root dir called plugin_options.name
            copy_file(repo,  filename, contents)
        elsif is_compiler_file encoded_contents
            copy_file(repo, "compiler/" + filename, contents)
        elsif is_keymap_file encoded_contents
            copy_file(repo, "keymap/" + filename, contents)
        else
            case script['script_type']
            when 'color scheme' then copy_file(repo, "colors/" + filename, contents)
            when 'ftplugin' then copy_file(repo, "ftplugin/" + filename, contents)
            when 'game' then copy_file(repo, "plugin/" + filename, contents)
            when 'indent' then copy_file(repo, "indent/" + filename, contents)
            when 'syntax' then copy_file(repo, "syntax/" + filename, contents)
            when 'utility' then copy_file(repo, "plugin/" + filename, contents)
            when 'patch' then copy_file(repo, "plugin/" + filename, contents)
            else
                # if this fires, they must have added more script types?!
                raise "Don't know where to put #{filename} for #{script['script_type']}"
            end
        end
    elsif filename =~ /^[^\/]+\.txt$/
        # docfile in the root directory
        copy_file(repo, "doc/" + filename, contents)
    elsif filename =~ /(autoload|after)\/(#{$vimdirs.join('|')})\/([^\/]+)$/
        # vimdir in autoload or after: a/b/autoload/plugin/fixit.vim
        copy_file(repo, "#{$1}/#{$2}/#{$3}", contents)
    elsif filename =~ /^[^\/]+\/(#{$vimdirs.join('|')})\/([^\/]+)$/
        # developer put vimfiles in a subdir, i.e. fixit/plugin/fixit.vim.
        copy_file(repo, "#{$1}/#{$2}", contents)
    else
        copy_file(repo, filename, contents)
    end
end


def common_prefix set 
    # http://stackoverflow.com/questions/1916218/find-the-longest-common-starting-substring-in-a-set-of-strings
    chars = set.map {|w| w.split('') }
    chars[0].zip(*chars[1..-1]).map { |c| c.uniq }.take_while { |c| c.size == 1 }.join
end


def copy_filesystem repo, script, dir, opts={}
    paths = []
    Find.find(dir) do |path|
        if test(?l, path) or path =~ /(?:^|\/)(\.git|.hg|\.bzr|\.svn)$/i
            Find.prune
        else
            # make sure all subdirs are readable
            File.chmod 0700, path if test ?d, path
            # only work on files, and ignore anything that the caller says should be skipped
            paths << path if test(?f, path) && (block_given? ? !yield(path) : true)
        end
    end

    return if paths.empty?    # script 1433 is all directories, no files!
    prefix = paths.count == 1 ?  File.dirname(paths.first) : common_prefix(paths)
    prefix.sub! /\/+[^\/]*$/, '' unless test ?d, prefix  # trim any partial filenames
    prefix.sub! /(?:^|\/)#{Regexp.union $vimdirs}(?:\/.*|$)/, ''  # don't trim any vim dirs

    paths.each do |path|
        # make all files rw and preserve the executable bit
        mode = File::Stat.new(path).mode
        File.chmod 0600 | (mode & 0700), path

        if opts[:smart]
            # trim as much as we can off the front of the path
            localpath = path.sub /^#{Regexp.escape prefix}\/*/, ''
        else
            # use the archive location as the path
            localpath = path.sub /^#{Regexp.escape dir}\/*/, ''
        end

        if opts[:smart]
            smart_copy_file repo, script, localpath, File.read(path)
        else
            copy_file repo, localpath, File.read(path)
        end
    end
end


def corrupt_vimball where
    puts "  corrupt vimball at #{where}"
    throw :corrupt
end


def unvimball repo, script, vimball
    # this routine deeply inspired by
    # https://github.com/carlhuda/janus/commit/6ae56116aa9f1cd65af43bd550405295ff2f5967
    lines = File.readlines(vimball).map &:chomp

    # If UseVimball and finish headers aren't found, archive is corrupt
    # This should be /^" Vimball Archiver/ but the BOM screws Ruby up.
    corrupt_vimball "A" unless lines.shift =~ /" Vimball Archiver/
    until (current = lines.shift) =~ /UseVimball\s*$/
        corrupt_vimball "B" unless current =~ /^\s*("|$)/
    end
    until (current = lines.shift) =~ /finish\s*$/
        corrupt_vimball "C" unless current =~ /^\s*("|$)/
    end

    while current = lines.shift
        # filename (followed by who knows), then the number of lines, then the data
        path = current
        path.sub! /\t\[\[\[1$/, ''
        path.gsub! '\\', '/'
        current = lines.shift
        num_lines = current[/^(\d+)$/, 1].to_i
        data = lines.slice!(0, num_lines)

        unless path == 'doc/tags'
            path = cleanpath path
            path = fix_file_location(script, path) || path
            copy_file repo, path, data.join("\n")+"\n" unless path == true
        end
    end
end


def unshell repo, script, localpath, cmd
    Dir.mktmpdir('scraper') do |tmpdir|
        fullpath = File.expand_path(localpath)
        Dir.chdir(tmpdir) do
            cmd = [*cmd, fullpath]
            # unzip returns a 1 exit code for success with warnings
            unless system(*cmd) || (cmd[0] == 'unzip' && $?.exitstatus == 1)
                raise "couldn't run #{cmd.join(' ')}: #{$?}"
            end
        end
        copy_filesystem repo, script, tmpdir, :smart => true
    end
end


def ungzip file
    Zlib::GzipReader.open(file) { |gz| yield(gz) }
end


def unbzip2 file
    Bzip2::Reader.open(file) { |bz| yield(bz) }
end


def unxz file
      IO.popen(['xz', '-d', '--to-stdout', file]) do |contents|
          yield(contents)
      end
end


def download_package version, script
    pkgname = "#{version['date']} #{filenameify(script_version(version))} #{filenameify(version['filename'])}"
    pkgdir = File.join($packages_dir, "#{'%04d' % script['script_id']} - #{filenameify(script['name'])}")
    Dir.mkdir pkgdir unless test ?d, pkgdir
    pkgfile = File.join(pkgdir, pkgname)
    download_file(version['url'], pkgfile) unless test ?f, pkgfile
    return pkgfile
end


def sense_zipped_file repo, script, filename, infile
    # sense the payload of a compressed file.  don't want to recurse into sense_file: too complex.
    contents = infile.read
    contents.force_encoding('ASCII-8BIT')
    Tempfile.open('scraper') do |ttfile|
        ttfile.write(contents)
        ttfile.close

        if MimeMagic.by_magic(contents) == 'application/x-tar'
            unshell repo, script, ttfile.path, ['tar', 'xf']
        elsif contents[0..512] =~ /^\bUseVimball\s*$/
            unvimball repo, script, ttfile.path
        else
            smart_copy_file repo, script, filename, contents
        end
    end
end


def is_some_sort_of_zipfile type
    type.include?('application/zip') || type.include?('application/x-gzip') || type.include?('application/x-bzip')
end


def sense_file repo, actual_name, pkgfile
    # some files lie about their type (claim to be .zips but are just .vim files).  fix em.
    extension_type = MIME::Types.type_for(actual_name)
    magic_type = File.open(pkgfile) { |f| MimeMagic.by_magic(f) }
    if ( extension_type.include?(magic_type) ||
        (extension_type.include?('application/x-bzip2') && magic_type == 'application/x-bzip') ||
        (extension_type.include?('application/x-rar-compressed') && magic_type == 'application/x-rar') )
        # we're good, extension and magic match up
    else
        # need to figure out what's going on and fix it.
        if (magic_type.nil? || magic_type.text?) && actual_name =~ /#{$textext}/
            # no problem, it's a textfile
        elsif actual_name =~ /\.vba$|\.vimball$/
            # arg, a vimball.  let it through.  see sense_zipped_file for vimball sensing.
        elsif actual_name =~ /^[^a-z]*\.VIM$/
            # i guess dos users might do everything in caps
            actual_name.downcase!
        elsif actual_name =~ /^(.*)\.VIM$/
            actual_name = "#{$1}.vim"
        elsif ( %w{application/x-awk application/x-perl application/x-ruby application/x-shellscript application/xml}.include?(magic_type.to_s) ||
                (magic_type == nil && %w{vimopen cleanswap cvsvimdiff vmake vim_menu_HTMLpol}.include?(actual_name)) ||
                %w{zshrc vimrc _vimrc .vimrc}.include?(actual_name) ||
                actual_name =~ /\.dict$/ || %w{pydiction xdebug2}.include?(actual_name) ||  # dictionaries
                actual_name =~ /\.applescript$/ ||
                magic_type == 'application/x-java' ||
                magic_type == 'application/x-ms-dos-executable' )
            # names and magic have failed us, copy these files over raw
            copy_file(repo, actual_name, File.read(pkgfile))
            actual_name = nil
        elsif magic_type == 'application/zip' && extension_type.include?('application/x-java-archive')
            # jarfiles sense as zipfiles so don't let the fixup code below "correct" the package extension
        elsif magic_type == 'application/zip'
        # authors have uploaded a lot of compressed archives without the correct extension.
            actual_name += '.zip'
        elsif magic_type == 'application/x-gzip'
            actual_name += '.gz'
        elsif magic_type == 'application/x-bzip'
            actual_name += '.bz2'
        elsif magic_type == 'application/x-7z-compressed'
            actual_name += '.7z'
        elsif magic_type == 'application/x-xz'
            actual_name += '.xz'
        elsif is_some_sort_of_zipfile(extension_type)
            # extension claims zipfile but magic disagrees, this happens a lot
            actual_name.sub!(/\.zip$|\.tar\.gz$|\.tar\.bz2?$|\.tgz$|\.tbz2?$/i, '')
            if magic_type == 'text/x-python'
                actual_name += '.py'
            elsif magic_type.nil? || magic_type.text?   # chances are it's a vimscript...?
                actual_name += '.vim'   # http://www.vim.org/scripts/script.php?script_id=29
            elsif magic_type == 'application/x-tar'
                actual_name += '.tar'
            else
                # just need to hope that nobody makes this mistake anymore.  if they do, fix this function.
                raise "unknown failed zip type for #{actual_name}: #{magic_type}"
            end
        elsif magic_type == 'application/x-macbinary'
            copy_file(repo, actual_name + '.macbinary', File.read(pkgfile))
            actual_name = nil
        elsif !magic_type && actual_name =~ /^exUtility-[0-9.]*.tar$/
            # odd that magic couldn't sense this valid tarfile.  owell, process it as normal.
        elsif extension_type.include?('application/x-tar') && magic_type == nil
            # magic has a bug where tarfiles are missed: http://github.com/minad/mimemagic/issues/#issue/1
            # when this happens, we just blindly trust the extension.
        else
            # there's such a range of reasons why this will happen that there's nothing to do but
            # have a human improve this function to handle this case.  :(
            raise "differing mime types for #{actual_name}, ext claims #{extension_type} but magic is #{magic_type.inspect}"
        end
    end
    return actual_name
end


def add_version repo, version, script
    # adds all the files in the package to the repo
    pkgfile = download_package(version, script)
    actual_name = sense_file(repo, version['filename'], pkgfile)

    case actual_name
    when nil then # do nothing
    when /#{$textext}/ then smart_copy_file repo, script, actual_name, File.read(pkgfile)
    when /\.zip$/i then unshell repo, script, pkgfile, ['unzip']
    when /\.tar$/ then unshell repo, script, pkgfile, ['tar', 'xf']
    when /\.t?gz$/i then ungzip(pkgfile) { |contents| sense_zipped_file(repo, script, version['filename'].sub(/\.t?gz$/, ''), contents) }
    when /\.t?bz2?$/i then unbzip2(pkgfile) { |contents| sense_zipped_file(repo, script, version['filename'].sub(/\.t?bz2?$/, ''), contents) }
    when /\.xz$/ then unxz(pkgfile) { |contents| sense_zipped_file(repo, script, version['filename'].sub(/\.xz$/, ''), contents) }
    when /\.7z$/ then unshell repo, script, pkgfile, ['7za', 'x']
    when /\.rar$/ then unshell repo, script, pkgfile, ['unrar', 'x']
    when /\.vba$|\.vimball$/ then unvimball repo, script, pkgfile
    when /\.jar$/ then copy_file repo, actual_name, File.read(pkgfile)
    when /\.gif$|\.jpe?g$|\.png$/ then copy_file repo, actual_name, File.read(pkgfile)
    else
        # probably need to add a new text file extension to $textext.
        # if not, it's probably a new compression format.
        raise "unknown filetype: #{actual_name}"
    end
end


def tag_version repo, version, branch
    ENV['GIT_COMMITTER_NAME'] = $vimscripts_name
    ENV['GIT_COMMITTER_EMAIL'] = $vimscripts_email
    sver = script_version(version)
    repo.git_tag '-a', gittagify(sver), '-m', "tag #{sver}", branch
    ENV.delete 'GIT_COMMITTER_NAME'
    ENV.delete 'GIT_COMMITTER_EMAIL'
end


def find_version repo, version
    tagname = gittagify(script_version(version))
    # gitrb doesn't handle annotated tags so we call git directly
    tag = repo.git_tag('-l', tagname).chomp
    tag = nil if tag =~ /^\s*$/
    return tag
end


def check_for_readme_file repo
    # we drop a README file into each repo.  don't want to conflict with one already there.
    repo.root.to_a.each do |name, value|
        if name =~ /^README$/i
            raise "already have a readme.orig!" if repo.root.to_a.find { |n,v| n =~ /^readme\.orig$/i }
            repo.root[name + '.orig'] = repo.root.delete(name)
        end
    end
end


def check_for_keymap_helper repo, script
    # It's easy to autodetect keymap files (see is_keymap_file above)
    # but some keymaps come with additional scripts that are not so common
    return unless repo.root['keymap'] && repo.root['plugin']
    names = repo.root['keymap'].map { |n,v| n }
    namesre = Regexp.union names
    repo.root['plugin'].each do |name, node|
        contents = node.data.dup
        contents.force_encoding "ASCII-8BIT"
        if contents.lines.find { |line| line =~ /^\s*source\s+<sfile>:p:h\/#{namesre}/ }
            # yep, it's a keymap helper.
            blob = repo.root['plugin'].delete(name)
            copy_file repo, "keymap/#{name}", blob.data
        end
    end
end


def corrupted_package script, version
    # returns true if this version has been hard-coded as corrupted
    names = $skip_packages[script['script_id'].to_i]
    names && [names].flatten.include?("#{version['date']} #{version['script_version']}")
end


def store_versions_in_repo repo, script
    committer = Gitrb::User.new($vimscripts_name, $vimscripts_email)
    puts "Processing script #{script['script_id']}: #{script['name']}"
    count = 0
    script['versions'].reverse.each do |version|
        branch = 'master'
        matcher = $branch_versions[script['script_id'].to_i]
        if matcher && version['filename'] =~ matcher[:regex]
            branch = matcher[:branch]
        end
        repo.branch = branch unless repo.branch == branch

        author_name, author_email = fix_email_address(version['author'])
        author = Gitrb::User.new(author_name, author_email, Time.new(*version['date'].split('-'), 0, 0, 0, 0))
        unless corrupted_package(script, version) || find_version(repo, version)
            catch :corrupt do
                puts "  adding #{version['filename']} #{version['date']} #{script_version(version)} to branch #{branch}"
                repo.transaction(fix_release_notes(version), author, committer) do
                    # delete all existing blobs since we replace everything with the new commit
                    repo.root.to_a.map { |name,value| repo.root.delete(name) }
                    add_version repo, version, script
                    check_for_keymap_helper repo, script
                    check_for_readme_file repo
                    copy_file(repo, 'README', "This is a mirror of #{script_id_to_url(script['script_id'])}\n\n" + script['description'] + "\n") unless repo.root['README']
                end
                tag_version repo, version, branch
                count += 1
            end
        end
    end
    count
end


def repo_url script
    "http://github.com/vim-scripts/#{script['name']}"
end


# push to vim-scripts.github.com so it we don't interfere with your regular ssh key.
# create a ~/.ssh/vimscripts-id_rsa and ~/.ssh/vimscripts-id_rsa.pub keypair,
# and create a ~/.ssh/config that has 2 Host sections:
#   Host github.com\nHostName github.com\nUser git\nIdentityFile ~/.ssh/id_rsa
#   Host vim-scripts.github.com\nHostName github.com\nUser git\nIdentityFile ~/.ssh/vimscripts-id_rsa
# see this for more: http://help.github.com/multiple-keys

def remote_url script
    "git@vim-scripts.github.com:vim-scripts/#{script['name']}"
end


def start_selenium
    sel = Selenium::Client::Driver.new :host => 'localhost',
          :port => 4444, :browser => 'firefox', :url => 'https://github.com'
    sel.start
    sel.set_context "deleee"
    sel.open "/login"
    sel.type "login_field", "vim-scripts"
    password = File.read('password').chomp rescue raise("Put vim-script's password in a file named 'password'.")
    sel.type "password", password
    sel.click "commit", :wait_for => :page
    sel
end


# github's api is claiming some repos exist when they clearly don't.  the
# only way to fix this appears to be to create a repo of the same name and
# delete it using the regular interface (trying to delete using the api
# throws 500 server errors).  Hence all this Selenium.  Arg.

def obliterate_repo sel, name
    sel.open "/repositories/new"
    sel.type "repository_name", name
    sel.click "//button[@type='submit']", :wait_for => :page
    sel.open "/vim-scripts/#{name}/admin"
    sel.click "//div[@id='addons_bucket']/div[3]/div[1]/a/span"
    sel.click "//div[@id='addons_bucket']/div[3]/div[3]/form/button"
end


def repo_heads repo
    path = "#{repo.path}/refs/heads"
    Dir.entries(path).select { |f|
        test ?f, "#{path}/#{f}"
    }
end


def perform_push repo_name
    return unless repo_name
    creds = Hashie::Mash.new(JSON.parse(File.read('creds.json')))
    github = Octopussy::Client.new(creds)
    repo = Gitrb::Repository.new(:path => repo_name.dup, :bare => true)
    script = JSON.parse(File.read(File.join(repo_name, $git_script_file)))
    puts "Uploading #{script['script_id']} - #{script['name']}"

    start = Time.now
    api_calls = 0

    remote = github.repo("vim-scripts/#{script['name']}") rescue nil
    api_calls += 1

    # if selenium is true then we must be having problems with phantom repos
    if remote && $selenium
        puts "  apparently #{remote.url} exists, obliterating..."
        obliterate_repo $selenium, script['name']
        remote = nil
        puts "  obliterate succeeded."
        sleep 2  # github requires a bit of time to sync
    end

    if remote
        # make sure this actually is the same repo
        puts "  remote already exists: #{remote.url}"
        remote.homepage =~ /script_id=(\d+)$/
        raise "bad url on github repo #{script['name']}" unless $1
        raise "remote #{script['name']} exists but id is for #{$1}" if script['script_id'] != $1
    else
        puts "  remote doesn't exist, creating..."
    end

    unless remote
        retryable(:tries => 4, :sleep => 10) do |retries|
            puts "  creating #{script['script_id']} - #{script['name']}#{retries > 0 ? "  TRY #{retries}" : ""}"
            remote = github.create(
                :name => script['name'],
                :description => "#{script['summary']}",
                :homepage => script_id_to_url(script['script_id']),
                :public => true)
        end
        api_calls += 1

        # turn off the issues and wiki tabs (wish 'create' would do that)
        retryable(:tries => 4, :sleep => 10) do |retries|
            puts "  disabling wiki+issues for #{script['script_id']} - #{script['name']}#{retries > 0 ? "  TRY #{retries}" : ""}"
            github.set_repo_info('vim-scripts/' + script['name'], :has_issues => false, :has_wiki => false)
        end
        api_calls += 1
    end

    repo.git_remote('rm', 'origin') rescue nil
    repo.git_remote('add', 'origin', remote_url(script))
    retryable(:tries => 6, :sleep => 15) do |retries|
        # Gitrb::CommandError is as close to a network timeout error as we're going to get
        puts "  #{"force " if ENV['FORCE']}pushing #{script['script_id']} - #{script['name']}#{retries > 0 ? "  TRY #{retries}" : ""}"
        args = ['--tags']
        args << '--force' if ENV['FORCE']
        args << 'origin'
        args.push *repo_heads(repo)
        repo.git_push(*args)
    end

    # sleep to avoid bumping into github's 60-per-minute API limit
    # the push doesn't count toward the API limit
    stop = Time.now
    sleep api_calls-(stop-start) if stop-start < api_calls

    # we don't want any provision for forcing pushes.  if you delete
    # and recreate a repo, you must manually delete and recreate it
    # on github.  forcing is bad, avoid at all costs.

    # we should have a script that will compare the full list of
    # repos on github and here and print any differences.  that is
    # not a part of this script's job.
    # Octopussy.list_repos('vim-scripts')
    # Octopussy.delete("vim-scripts/#{ghname}")

    # no need to reset the remote because presumably we created this
    # repo and the remote is already set correctly.
end


def perform_scrape script_id
    script = scrape_script script_id
    script = resolve_name_conflicts script
    script = resolve_renamed_scripts script
    write_script script
end


def perform_download script_file
    return [nil,-1] unless script_file
    script = JSON.parse(File.read(script_file))
    repo = open_repo script['script_id'], script['name']

    dedup_script_versions script

    # store the script file in the repo it creates
    File.open(File.join(repo.path, $git_script_file), 'w') { |f|
        f.write JSON.pretty_generate(script) + "\n"
    }

    count = store_versions_in_repo(repo, script)
    dump_repo "#{$repo_dir}/#{repo.path}", ENV['DUMP_REPO'] if ENV['DUMP_REPO']
    FileUtils.rm_rf "#{$repo_dir}/#{repo.path}" if ENV['PURGE_REPOS']
    [repo.path, count]
end


def dump_repo repo, stats_dir
    raise "no repo" unless repo
    repo = repo.gsub /\/$/, ''  # remove trailing / added by filename completion
    ENV['GIT_DIR'] = repo
    log = `git log --pretty=fuller --decorate=full --stat`
    # remove every line with a CommitDate since that's horribly variable
    log = log.each_line.reduce([]) { |a,v|
        v.gsub!(/^commit\s+[0-9A-Fa-f]*/, 'commit SHA1SHA1SHA1');
        v.gsub!(/, refs\/(remotes|heads)[^,)]*\/master/, '');
        a.push(v) unless v =~ /^CommitDate: /;
        a }.join
    ENV.delete('GIT_DIR')
    filename = File.join(stats_dir, File.basename(repo).gsub(/\.git$/, '') + '.stats')
    puts "Stats in #{filename}"
    File.open(filename, 'w') { |f|
        f.write log
    }
end


def perform_all script_id, always_push=false
    repo_name,count = perform_download(perform_scrape(script_id.to_s))
    if $pushing && (always_push or count > 0)
        perform_push repo_name
    end
    [repo_name, count]
end


def fetch_rss
    feed = Feedzirra::Feed.fetch_and_parse($rss_url)
    puts "feed contains #{feed.entries.map { |e| script_id_from_url(e.url) }.inspect}"
    feed
end


def timeout_expired?
    if Time.now > $start_time + $max_run_time
        puts "We are now #{Time.now - ($start_time + $max_run_time)} seconds past time to quit."
        true
    else
        false
    end
end


def enter_full_mode state
    state.mode = 'full'
    feed = fetch_rss
    state.target_rss_id = feed.entries.first.entry_id
    state.full_index = 1
    puts "Entering full scrape mode!  Target is #{script_id_from_url feed.entries.first.url} / #{feed.entries.first.entry_id}."
end


def enter_rss_mode state
    state.last_rss_id = state.target_rss_id
    state.target_rss_id = nil
    state.full_index = nil
    state.mode = 'rss'
    puts "Entering RSS scrape mode!"
end


def perform_full
    state = read_state

    # The target rss id is the most recent rss id when we start the full scrape.
    # If it's still in the feed when we finish the full scrape, we're synced
    # and can return to rss mode.  If not, the full scrape must start again.
    if state.target_rss_id.nil? || state.mode != 'full'
        enter_full_mode state
    else
        puts "Resuming full scrape from #{state.full_index}.  Target is #{state.target_rss_id}."
    end

    highest_index = highest_script_id
    original_index = state.full_index
    while true
        break if timeout_expired?
        # Always push the first repo for the same reason as in perform_rss.
        # Only push subsequent repos if we actually downloaded changes because
        # pushing takes a lot of time, even if there's nothing to push.
        repo_name,count = perform_all(state.full_index, state.full_index == original_index)
        if repo_name.nil? && state.full_index > highest_index + 4
            # full scrape succeeded!
            enter_rss_mode state
            write_state state
            # do an rss scrape so we don't lose sync while waiting for the cronjob to fire
            perform_rss false
            generate_docs
            break
        end
        state.full_index += 1
        write_state state
    end
end


def perform_rss fallback=true
    state = read_state
    if state.mode != 'rss' || !state.last_rss_id
        perform_full
        return nil
    else
        puts "Resuming RSS scrape."
    end

    feed = fetch_rss
    entries = nil

    last_index = feed.entries.index { |e| e.entry_id == state.last_rss_id }
    if last_index
        if last_index == 0
            # rss is up to date!  nothing to scrape
            entries = []
        else
            entries = feed.entries[0..(last_index - 1)]
        end
    else
        puts "can't find  previous location #{state.last_rss_id} in the RSS feed."
        if fallback
            puts "Performing a full scrape from the beginning!"
            perform_full
        end
        entries = nil
    end

    if entries
        puts "   processing #{entries.map { |e| script_id_from_url(e.url) }.inspect}"
        entries.reverse.each do |entry|
            break if timeout_expired?
            script_id = script_id_from_url(entry.url)
            puts "Fetching script #{script_id} for rss #{entry.entry_id}"

            # always_push=true because there's a chance the previous scrape failed due to
            # a transient network error.  next time we run, we may find that there are
            # no versions to download, but the repo still needs to be pushed.
            perform_all(script_id, true)

            state.last_rss_id = entry.entry_id
            write_state state
        end
        generate_docs
    end

    entries ? entries.count : nil
end


def perform_continuous count
    state = read_state
    state.last_script_id ||= 0
    state.last_script_id += 1

    # keep looking past the last known id in case rss missed new scripts
    if state.last_script_id > highest_script_id + 3
        puts "Count is #{state.last_script_id} and the highest known script id is #{highest_script_id}.  Starting from 0!"
        state.last_script_id = 1
    end

    puts "Performing continuous scrape of #{state.last_script_id} through #{state.last_script_id+count-1}"
    state.last_script_id.upto(state.last_script_id+count-1) do |script_id|
        break if timeout_expired?
        puts "Fetching script #{script_id} for continuous scrape"
        perform_all(script_id, true)
        write_state state
    end
end


def fork_jobs args
    # This helps a lot (14 mins to 9 mins) but could even be better.
    # If 3 jobs are forked, 0-1100 gets done in 60% of the time of 2200-3300.
    # Might fork batches of 10 at a time or something...  not sure it's
    # worth the additional complexity just to save 30 sec though.
    #   todo: what about randomizing the argument order before grouping them?
    if ENV['JOBS']
        ENV['JOBS'].to_i.downto(2) do |i|
            workunit = args.count / i
            if i > 1 && workunit > 0 && fork.nil?
                args = args[0..workunit-1]
                $child_number = i
                break
            else # parent
                args = args[workunit..-1]
            end
        end
        puts "Processing #{args.size} unit#{'s' if args.size != 1}."
    end
    args
end


$start_time = Time.now

if ARGV.empty?
    puts "Starting scrape at #{$start_time}"
    # don't time out after 15 minutes if FOREVER=1
    $max_run_time = 10.days if ENV['FOREVER']
    count = perform_rss
    # also cycle through all packages in case the rss feed missed anything
    perform_continuous($idle_count - count) if count && $idle_count - count > 0
elsif ARGV[0] == '--docs'
    generate_docs
elsif ARGV[0] == '--dump'
    ARGV[1..-1].each { |repo| dump_repo repo, 'scraper-stats' }
elsif ARGV[0] == '--all'
    # this generates all the repos but doesn't push them
    $max_run_time = 10.days
    $pushing = false
    perform_full
else
    args = fork_jobs ARGV
    args.each do |arg|
        # if you're passing args, you probably don't want the scraper to quit
        # before it's finished processing them all.
        $max_run_time = 10.days

        if arg =~ /^\d+$/
            # leading zeros cause script ID to be interpreted as octal
            perform_all arg.gsub(/^0+/, ''), true
        elsif arg =~ /^-(\d+)$/
            # leading zeros cause script ID to be interpreted as octal
            perform_scrape $1.gsub(/^0+/, '')
        elsif test ?f, arg
            perform_download arg
        elsif test ?d, arg
            # the github api appears to have bugs, see obliterate_repo for more.
            # if you're having troubles where the api claims a repo exists but can't
            # push to it, and the regular user interface claims it doesn't exist,
            # use selenium to create & delete the repo.  It appears to fix things.
            #      to use selenium:   OBLITERATE=1 ./scraper repos/*
            $selenium = start_selenium if ENV['OBLITERATE'] && !$selenium
            retryable(:tries => 6, :sleep => 90) do |retries|
                # sometimes, and it's hard to predict why, github errors out reliably
                # for a few minutes, then magically fixes itself.
                puts "Upload failed, retry #{retries}" if retries > 0
                perform_push arg
            end
        elsif (f=Dir.glob("#{$scripts_dir}/???? - #{arg}.json")) && !f.empty? && test(?f, f.first)
            # plain script name -- must have already been scraped tho
            raise "multiple hits for #{arg}: #{f.inspect}" unless f.count == 1
            f.first =~ /^#{$scripts_dir}\/(\d\d\d\d)/
            perform_all $1.gsub(/^0+/, ''), true
        else
            raise "Could not recognize argument #{arg}"
        end
    end

    unless $child_number || args.count == ARGV.count
        puts "waiting for children to finish..."
        Process.wait
    end
end

if $selenium
    $selenium.close_current_browser_session
    $selenium.stop
end

stop = Time.now
puts "done#{" #{$child_number}" if $child_number}.  Run took #{'%.2f' % (stop-$start_time)} seconds or #{ '%.3f' % ((stop-$start_time)/60)} minutes"

